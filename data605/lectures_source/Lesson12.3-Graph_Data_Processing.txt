// Dir is https://drive.google.com/drive/folders/1u8ZUAkLc8yZBwGgXvfBcAY_oSCyzT_pp
// 
// 

::: columns
:::: {.column width=15%}
![](data605/lectures_source/images/UMD_Logo.png)
::::
:::: {.column width=75%}

\vspace{0.4cm}
\begingroup \large
UMD DATA605 - Big Data Systems
\endgroup
::::
:::

\vspace{1cm}

\begingroup \Large
**$$\text{\blue{12.3: Graph Data Processing}}$$**
\endgroup

::: columns
:::: {.column width=75%}
- **Instructor**: Dr. GP Saggese, [gsaggese@umd.edu](gsaggese@umd.edu)

::::
:::: {.column width=20%}
::::
:::

* Options for Processing Graph Data 
- Write your own programs
  - Extract relevant data, construct in-memory graph
  - Different storage options help to varying degrees

- Write queries in a declarative language
  - Suitable for some graph queries/tasks
  - E.g., Cypher for Neo4j

- Use a general-purpose distributed programming framework
  - E.g., Hadoop or Spark
  - Difficult for many graph analysis tasks

- Use a graph-specific programming framework
  - Simplifies writing graph analysis tasks, scales to large volumes
  - E.g., Giraph or GraphX

* Option 2: Declarative Interfaces 
- No consensus on declarative, high-level languages for querying or analysis
  - Variety in query/analysis tasks
  - Hard to find and exploit commonalities

- Limited, useful solutions:
  - XQuery for XML
    - Limited to tree-structured data
  - SPARQL for RDF
    - Standardized query language, limited functionality
  - Cypher by Neo4j
  - Datalog-based frameworks for analysis tasks
    - Many prototypes, task-specific

* Option 3: MapReduce 
- Popular option for processing large datasets
  - Hadoop or Spark

- Key advantages:
  - Scalability without scheduling, distributed execution, fault tolerance
    concerns
  - Simple programming framework

- Disadvantages:
  - Difficult for graph analysis tasks
  - Each traversal requires a new map-reduce phase
    - Hadoop not ideal for many phases, Spark is better

- Much work on graph analysis tasks using MapReduce

* Option 3: MapReduce 
- Disadvantages:
  - Difficult for graph analysis
  - Each traversal requires a new map-reduce phase
    - Each job executes _N_ times
  - Hadoop not ideal for many phases (even with YARN)
  - Inefficient – redundant work
    - Mappers send PR values and graph structure
  - In PageRank: repeated reading and parsing each iteration
    - Extensive I/O at input, shuffle/sort, output

* Option 4: Graph Programming Frameworks 
- Frameworks (analogous to MapReduce) for analyzing large graph data
  - Address MapReduce limitations
  - Most are _vertex-centric_
    - Programs from a vertex perspective
  - Based on message passing between nodes

- Pregel: original framework by Google
  - Based on "Bulk Synchronous Parallel" (BSP) model

- Giraph: open-source Pregel on Hadoop

- GraphLab: asynchronous execution

- GraphX: built on Spark

* Bulk Synchronous Parallel (BSP) 

![](data605/lectures_source/images/lecture_12_3/lec_12_3_slide_7_image_1.png)

time

* Vertex-centric BSP 
- Each vertex has an id, value, list of adjacent vertex ids, and edge values

- Each vertex invoked in each superstep, recomputes value, sends messages to
  other vertices, delivered over superstep barriers

- Advanced features: termination votes, combiners, aggregators, topology
  mutations

![](data605/lectures_source/images/lecture_12_3/lec_12_3_slide_8_image_1.png)

* Think like a vertex
- I know my local state
- I know my neighbours
- I can send messages to vertices
- I can declare that I am done
- I can mutate graph topology

* Option 4: Pregel
- Programmers write one program: `compute()`

- Typical structure of `compute()`:
  - _Inputs_: current values of the node
  - _Inputs_: messages from neighboring nodes
  - Modify current values (if desired)
  - _Outputs_: send messages to neighbors

- Execution framework:
  - Execute _compute()_ for all nodes in parallel
  - Synchronize (wait for all messages)
  - Repeat

* Apache Giraph
- Pregel is proprietary, but:
  - **Apache Giraph**: open source implementation
  - Runs on standard **Hadoop** infrastructure
  - Computation executed in memory
  - Can be a job in a pipeline (**MapReduce, Hive**)
  - Uses **Apache ZooKeeper** for synchronization
  - Graph partition via hashing
  - Fault tolerance via checkpointing

* Plays well with Hadoop

![](data605/lectures_source/images/lecture_12_3/lec_12_3_slide_13_image_1.png)

* Giraph Execution

![](data605/lectures_source/images/lecture_12_3/lec_12_3_slide_14_image_1.png)

* Which part is doing what?
- **ZooKeeper**: responsible for computation state
  - partition/worker mapping
  - global state: #superstep
  - checkpoint paths, aggregator values, statistics

- **Master**: responsible for coordination
  - assigns partitions to workers
  - coordinates synchronization
  - requests checkpoints
  - aggregates aggregator values
  - collects health statuses

- **Worker**: responsible for vertices
  - invokes active vertices compute() function
  - sends, receives, assigns messages
  - computes local aggregation values

* What do you have to implement?
- Your algorithm as a **Vertex**
  - Subclass existing implementations: `BasicVertex, MutableVertex,
    EdgeListVertex, HashMapVertex, LongDoubleFloatDoubleVertex`

- A `VertexInputFormat` to read your graph
  - e.g., from a text file with adjacency lists like _<vertex> <neighbor1>
    <neighbor2> ..._

- A `VertexOutputFormat` to write back the result
  - e.g., _<vertex> <pageRank>_

* A vertex view 

![](data605/lectures_source/images/lecture_12_3/lec_12_3_slide_17_image_1.png)

* Designed for iterations 
- Stateful (in-memory)
  - Keep all data in memory if possible

- Only intermediate values (messages) sent
  - Communicate with other vertices

- Hits disk at input, output, checkpoint

- Can go out-of-core
  - If data doesn't fit into memory

* Graph modeling in Giraph 
- BasicComputation<          I extends WritableComparable, // VertexID -- vertex ref           V extends Writable, // VertexData -- a vertex datum           E extends Writable, // EdgeData -- an edge label           M extends Writable> // MessageData-– message payload 

* Giraph "Hello World" 
```java
public class GiraphHelloWorld extends    
BasicComputation<IntWritable, IntWritable, NullWritable, NullWritable> {

public void compute(Vertex<IntWritable, IntWritable,     NullWritable> vertex, Iterable<NullWritable> messages) { 
   System.out.println("Hello world from the: " + vertex.getId() + " : ");     for (Edge<IntWritable, NullWritable> e : vertex.getEdges()) { 
      System.out.println(" " + e.getTargetVertexId());     } 
   System.out.println("");     
   vertex.voteToHalt(); 
} 
}
```
* Example: Ping neighbors

```java
public void compute(Vertex<Text, DoubleWritable, DoubleWritable> vertex, Iterable<Text> ms ){ 
   if (getSuperstep() == 0) {        sendMessageToAllEdges(vertex, vertex.getId()); 
   } else {        for (Text m : ms) { 
          if (vertex.getEdgeValue(m) == null) {              vertex.addEdge(EdgeFactory.create(m, SYNTHETIC_EDGE));            }        } 
   }     vertex.voteToHalt(); 
}
```
* Giraph PageRank Example

```java
public class PageRankComputation      extends BasicComputation<IntWritable, FloatWritable, NullWritable, FloatWritable> {
  //Number of supersteps 
  public static final String SUPERSTEP_COUNT =              "giraph.pageRank.superstepCount";
```

* Giraph PageRank Example
```java
public void compute(Vertex<IntWritable, FloatWritable, NullWritable> vertex, Iterable<FloatWritable> messages)       throws IOException {
    if (getSuperstep() >= 1) {
      float sum = 0;
      for (FloatWritable message : messages) {
        sum += message.get();
      }
      vertex.getValue().set((0.15f / getTotalNumVertices()) +                              0.85f * sum);
    }
    if (getSuperstep()<getConf().getInt(SUPERSTEP_COUNT, 0)) {
      sendMessageToAllEdges(vertex,
          new FloatWritable(vertex.getValue().get() /                             vertex.getNumEdges()));
    } else {
      vertex.voteToHalt();
    }
  }
}
```

* Additional functionality
- Combiners
  - To minimize messages 
- Aggregators
  - global aggregations across vertices 
- MasterCompute
  - computation executed on master 
- WorkerContext
  - executed per worker task
- PartitionContext
  - executed per partition

* Giraph scales 

![](data605/lectures_source/images/lecture_12_3/lec_12_3_slide_25_image_1.png)

https://www.facebook.com/notes/facebook-engineering/scaling-apache-giraph-to-a-trillion-edges/10151617006153920

* Graphx

* GraphX Motivation

![](data605/lectures_source/images/lecture_12_3/lec_12_3_slide_27_image_1.png)

* GraphX Motivation

- Difficult to Program and Use 
- Users must ***Learn, Deploy, and Manage*** multiple systems 

![](data605/lectures_source/images/lecture_12_3/lec_12_3_slide_28_image_1.png)

- Leads to brittle and often complex interfaces 

* And Inefficient

- Extensive **data movement** and duplication across the network and file system 

![](data605/lectures_source/images/lecture_12_3/lec_12_3_slide_29_image_1.png)

- Limited reuse of internal data-structures across stages 

* The GraphX Unified Approach 
:::columns
::::{.column width=50%}
New API 
Blurs the distinction between *Tables* and *Graphs *
::::
::::{.column width=50%}
New System 
Combines Data-Parallel Graph-Parallel Systems 
::::
:::

![](data605/lectures_source/images/lecture_12_3/lec_12_3_slide_30_image_1.png)

Enables users to easily and efficiently express the entire graph analytics pipeline 

* Representation 

![](data605/lectures_source/images/lecture_12_3/lec_12_3_slide_31_image_1.png)

- Plus optimizations:
  - Distributed join optimization
  - Materialized view maintenance

* Graph modeling in GraphX
- The property graph is parameterized over the vertex (VD) and edge (ED) types 

```
     class Graph[VD, ED] {
       val vertices: VertexRDD[VD] 
       val edges: EdgeRDD[ED] 
     }
``` 
- Graph[(String, String), String] 
![](data605/lectures_source/images/lecture_12_3/lec_12_3_slide_32_image_1.png){width=60%}

* Creating a Graph (Scala)

:::columns
::::{.column width=50%}
\footnotesize
```
type VertexId = Long

val vertices: RDD[(VertexId, String)] =
sc.parallelize(List(
(1L,"Alice"),
(2L, "Bob"),
(3L, "Charlie")))

class Edge[ED](
val srcId: VertexId,
val dstId: VertexId,
val attr: ED)

val edges: RDD[Edge[String]] =
sc.parallelize(List(
Edge(1L, 2L, "coworker"),
Edge(2L, 3L, "friend")))

val graph = Graph(vertices, edges)
```
::::
::::{.column width=50%}
![](data605/lectures_source/images/lecture_12_3/lec_12_3_slide_33_image_1.png){width=70%}
::::
:::

* Hello world in GraphX
```
$ spark*/bin/spark-shell
scala> val inputFile = sc.textFile("hdfs:///tmp/graph/1.txt")
scala> val edges = inputFile.flatMap(s => {
val l = s.split("\t");
l.drop(1).map(x => (l.head.toLong, x.toLong))
})
scala> val graph = Graph.fromEdgeTuples(edges, "")
scala> val result = graph.collectNeighborIds(EdgeDirection.Out).map(x =>
println("Hello world from the: " + x._1 + " : " + x._2.mkString(" ")) )
scala> result.collect() // don't try this @home

Hello world from the: 1 :
Hello world from the: 2 : 1 3
Hello world from the: 3 : 1 2
```

* Spark Table Operators
- GraphX **Table** (RDD) operators are inherited from Spark:

:::columns
::::{.column width=33%}
- map
- filter
- groupBy
- sort
- union
- join
- leftOuterJoin
- rightOuterJoin
::::
::::{.column width=33%}
- reduce
- count
- fold
- reduceByKey 
- groupByKey
- cogroup
- cross
- zip
::::
::::{.column width=33%}
- sample
- take
- first
- partitionBy
- mapWith
- pipe
- save
- ...

::::
:::

* Graph Operators (Scala)
\footnotesize
```
class Graph [ V, E ] {
   def Graph(vertices: Table[ (Id, V) ], 
                    edges: Table[ (Id, Id, E) ]) 
   // Table Views ----------------- 
   def vertices: Table[ (Id, V) ]
   def edges: Table[ (Id, Id, E) ]
   def triplets: Table [ ((Id, V), (Id, V), E) ]
   // Transformations ------------------------------ 
   def reverse: Graph[V, E]
   def subgraph(pV: (Id, V) => Boolean, 
                         pE: Edge[V,E] => Boolean): Graph[V,E] 
   def mapV(m: (Id, V) => T ): Graph[T,E] 
   def mapE(m: Edge[V,E] => T ): Graph[V,T]
   // Joins ----------------------------------------
   def joinV(tbl: Table [(Id, T)]): Graph[(V, T), E ] 
   def joinE(tbl: Table [(Id, Id, T)]): Graph[V, (E, T)] 
   // Computation ----------------------------------
   def mrTriplets(mapF: (Edge[V,E]) => List[(Id, T)], 
                          reduceF: (T, T) => T): Graph[T, E] 
} 
```

* Built-in Algorithms (Scala)
\footnotesize
```
def pageRank(tol: Double): Graph[Double, Double]
def triangleCount(): Graph[Int, ED]
def connectedComponents(): Graph[VertexId, ED]
def stronglyConnectedComponents(numIter:Int):Graph[VertexID,ED]
// ...and more: org.apache.spark.graphx.lib (GraphX libraries)
```

![](data605/lectures_source/images/lecture_12_3/lec_12_3_slide_37_image_1.png)

* Triplets Join Vertices and Edges
- Triplets capture Gather-Scatter pattern from specialized graph processing systems (like Giraph)
- **Triplets** operator joins vertices and edges

![](data605/lectures_source/images/lecture_12_3/lec_12_3_slide_38_image_1.png)

* MapReduce Triplets
Map-Reduce triplets collect information about the neighborhood of each vertex:

![](data605/lectures_source/images/lecture_12_3/lec_12_3_slide_39_image_1.png)

* Performance Comparisons

![](data605/lectures_source/images/lecture_12_3/lec_12_3_slide_40_image_1.png)

* GraphX scales to larger graphs
![](data605/lectures_source/images/lecture_12_3/lec_12_3_slide_41_image_1.png)
- GraphX is roughly 2x slower than GraphLab
  - Scala + Java overhead: Lambdas, GC time, ...
  - No shared memory parallelism: 2x increase in communication

* But, a Small Pipeline in GraphX
Timed end-to-end GraphX is faster than GraphLab (and Giraph)

![](data605/lectures_source/images/lecture_12_3/lec_12_3_slide_42_image_1.png)

* Giraph vs. GraphX
:::columns
::::{.column width=50%}
- **Giraph**
  - An unconstrained BSP framework
  - Specialized fully mutable, dynamically balanced in-memory graph representation
  - Procedural, vertex-centric programming model
  - Part of Hadoop ecosystem
::::
::::{.column width=50%}
- **GraphX**
  - An RDD framework
  - Graphs are "views" on RDDs and thus immutable
  - Functional-like, "declarative" programming model
  - Genuine part of Spark ecosystem
::::
:::
