{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b5a0077",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    }
   ],
   "source": [
    "################################################################################################\n",
    "####        TITLE: TPOT (HARD)                                                              ####\n",
    "####        DESCRIPTION: FINANCIAL SENTIMENT ANALYSIS                                       ####\n",
    "####        AUTHOR: BRADLEY SCOTT                                                           ####\n",
    "####        UMD ID: 119 775 028                                                             ####\n",
    "####        DATE: 5NOV2025                                                                  ####\n",
    "####        REFERENCES USED (see paper for full details):                                   ####\n",
    "####            ChatGPT 5                                                                   ####\n",
    "####            FNSPID: A Comprehensive Financial News Dataset in Time Series               ####\n",
    "####        PYTHON VERSION: 3.11.14                                                         ####    \n",
    "################################################################################################\n",
    "'''\n",
    "[BS11052025] fp_610_000001\n",
    "[BS11052025] import all necessary modules\n",
    "'''\n",
    "import pandas as pd\n",
    "import os\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "import time\n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tpot import TPOTRegressor\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from dask.distributed import Client, LocalCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "502a1a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[BS11102025] fp_610_000005 \n",
    "[BS11102025] set file paths (local + Google Drive)\n",
    "'''\n",
    "# data directory\n",
    "BASE = Path(\"/workspace/data\") \n",
    "\n",
    "# --- Local file paths ---\n",
    "file_all = BASE / \"All_external.csv\"  # optional\n",
    "file_nq = BASE / \"nasdaq_exteral_data.csv\"\n",
    "file_daily_news = BASE / \"daily_news_sentiment.parquet\"\n",
    "file_val_tick = BASE / \"valid_tickers.csv\"\n",
    "\n",
    "# --- Google Drive files (public links) ---\n",
    "# To load directly from Google Drive, replace '/view' with '/uc?export=download&id='\n",
    "def gdrive_parquet_url(file_id: str) -> str:\n",
    "    return f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
    "\n",
    "file_prices_url = gdrive_parquet_url(\"1x4ayM-VwgKBUtaLixmo870-q3ddwLnPE\")\n",
    "file_model_url = gdrive_parquet_url(\"1PmZUpnlb1QBFr6-1s0bkk6wdt4uFMCIO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d506921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[BS11052025] fp_610_000010\\n[BS11052025] Get the first chunk of the nasdaq_exteral_data.csv file while testing code\\n    NB: all_external.csv does not have summaries and would need web scraping so not using that\\n        data at this time\\n    NB: This was fore testing purposes so commented out\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "[BS11052025] fp_610_000010\n",
    "[BS11052025] Get the first chunk of the nasdaq_exteral_data.csv file while testing code\n",
    "    NB: all_external.csv does not have summaries and would need web scraping so not using that\n",
    "        data at this time\n",
    "    NB: This was fore testing purposes so commented out\n",
    "'''\n",
    "#chunksize = 200_000 \n",
    "\n",
    "#reader = pd.read_csv(file_nq, chunksize=chunksize, encoding_errors=\"ignore\")\n",
    "\n",
    "#df_chunk = next(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6599ecca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[BS11052025] fp_610_000010b\\n[BS11052025] Exploring data. Commented out to keep code clean\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "[BS11052025] fp_610_000010b\n",
    "[BS11052025] Exploring data. Commented out to keep code clean\n",
    "'''\n",
    "# Display info and first few rows\n",
    "#print(\"Shape:\", df_chunk.shape)\n",
    "#print(\"\\nColumns:\\n\", df_chunk.columns.tolist())\n",
    "\n",
    "# Peek at a few rows\n",
    "#df_chunk.head(3)\n",
    "\n",
    "# Show which of those columns exist in your DataFrame\n",
    "#summary_cols = ['Lsa_summary', 'Luhn_summary', 'Textrank_summary', 'Lexrank_summary']\n",
    "#df_chunk[summary_cols].info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31c47e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[BS11052025] fp_610_000015\n",
    "[BS11052025] Build out functions to use VADER for sentiment analysis\n",
    "    NB: Will use textrank_summary if available, otherwise will use title\n",
    "'''\n",
    "tqdm.pandas()\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def pick_text(row):\n",
    "    # prefer Textrank_summary; fallback to title; finally to Article\n",
    "    for col in [\"Textrank_summary\", \"Article_title\", \"Article\"]:\n",
    "        if col in row and pd.notna(row[col]) and str(row[col]).strip():\n",
    "            return str(row[col])\n",
    "    return \"\"\n",
    "\n",
    "def vader_score(text: str) -> float:\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    return analyzer.polarity_scores(text)[\"compound\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cc05382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ daily_news_sentiment.parquet already exists ‚Äî skipping creation\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "[BS11052025] fp_610_000020\n",
    "[BS11052025] Do the sentiment analysis and build out daily_news_sentiment.parquet\n",
    "    NB: This took 50 minutes to run on my computer but it only needs to be ran once. \n",
    "'''\n",
    "chunksize = 200_000 \n",
    "\n",
    "# Only create the file if it doesn't already exist so we're not creating it more than once\n",
    "if not file_daily_news.exists():\n",
    "    print(\"üî® daily_news_sentiment.parquet does NOT exist ‚Äî creating it...\")\n",
    "\n",
    "    summary_cols = [\"Textrank_summary\"]   # suggest simplifying\n",
    "    base_cols = [\"Date\",\"Article_title\",\"Stock_symbol\",\"Article\"]\n",
    "    usecols = list(dict.fromkeys(base_cols + summary_cols))\n",
    "\n",
    "    writer = None\n",
    "\n",
    "    for i, chunk in enumerate(pd.read_csv(\n",
    "            file_nq, chunksize=chunksize, usecols=lambda c: True,\n",
    "            encoding_errors=\"ignore\"\n",
    "        )):\n",
    "\n",
    "        cols_present = [c for c in usecols if c in chunk.columns]\n",
    "        df = chunk[cols_present].copy()\n",
    "\n",
    "        # Clean date\n",
    "        date_col = \"Date\"\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\").dt.date\n",
    "        df = df.dropna(subset=[date_col])\n",
    "\n",
    "        # Expand tickers\n",
    "        df[\"Stock_symbol\"] = df[\"Stock_symbol\"].astype(str).str.replace(\" \", \"\")\n",
    "        df = df[df[\"Stock_symbol\"].str.len() > 0]\n",
    "        df = df.assign(Stock_symbol=df[\"Stock_symbol\"].str.split(\",\")).explode(\"Stock_symbol\")\n",
    "        df[\"Stock_symbol\"] = df[\"Stock_symbol\"].str.upper().str.replace(r\"[^A-Z\\.]\", \"\", regex=True)\n",
    "        df = df[df[\"Stock_symbol\"].str.len() > 0]\n",
    "\n",
    "        # Sentiment\n",
    "        df[\"_text\"] = df.apply(pick_text, axis=1)\n",
    "        df[\"_sent\"] = df[\"_text\"].map(vader_score).astype(\"float32\")\n",
    "\n",
    "        # Daily aggregation\n",
    "        daily = (\n",
    "            df.groupby([\"Stock_symbol\", date_col], as_index=False)\n",
    "              .agg(avg_sentiment=(\"_sent\",\"mean\"),\n",
    "                   pos_count=(\"_sent\", lambda s: np.sum(s > 0.05)),\n",
    "                   neg_count=(\"_sent\", lambda s: np.sum(s < -0.05)),\n",
    "                   news_count=(\"_text\",\"count\"))\n",
    "        )\n",
    "        daily = daily.rename(columns={date_col: \"date\", \"Stock_symbol\":\"ticker\"})\n",
    "\n",
    "        # Append to parquet\n",
    "        table = pa.Table.from_pandas(daily, preserve_index=False)\n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(file_daily_news, table.schema)\n",
    "        writer.write_table(table)\n",
    "\n",
    "        print(f\"Processed chunk {i}: wrote {len(daily)} daily rows\")\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "\n",
    "    print(\"‚úÖ File created:\", file_daily_news)\n",
    "\n",
    "else:\n",
    "    print(\"‚úÖ daily_news_sentiment.parquet already exists ‚Äî skipping creation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1989421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[BS11052025] fp_610_000020b\\n[BS11052025] Checking the file to see if there is any issues\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "[BS11052025] fp_610_000020b\n",
    "[BS11052025] Checking the file to see if there is any issues\n",
    "'''\n",
    "#df = pd.read_parquet(file_daily_news)\n",
    "#df.head(20)\n",
    "#len(df)\n",
    "#df['ticker'].nunique(), df['date'].nunique()\n",
    "#pd.set_option('display.max_rows', None)\n",
    "#df['ticker'].value_counts()\n",
    "#df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddfc9d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3578,\n",
       " ['GILD', 'BABA', 'MRK', 'CMCSA', 'KO', 'FDX', 'SLB', 'OXY', 'QQQ', 'NEE'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "[BS11072025] fp_610_000025\n",
    "[BS11052025] Load the sentiment analysis for any tickers with data for more than 200 days\n",
    "'''\n",
    "# Load daily sentiment (created earlier)\n",
    "news = pd.read_parquet(file_daily_news)\n",
    "\n",
    "# Format date\n",
    "news['date'] = pd.to_datetime(news['date']).dt.date\n",
    "\n",
    "# ---- ‚úÖ Filter to past 10 years ----\n",
    "news = news[(news['date'] >= datetime(2014,1,1).date())]\n",
    "\n",
    "# Clean ticker column: drop NaN and \"NAN\", \"\", etc.\n",
    "news = news[ news['ticker'].notna() ]               # removes actual NaN\n",
    "news = news[ news['ticker'].str.upper() != \"NAN\" ]  # removes literal \"NAN\"\n",
    "news = news[ news['ticker'].str.strip() != \"\" ]     # removes blanks\n",
    "\n",
    "# Remove invalid tickers (must be A‚ÄìZ or dot)\n",
    "is_valid = news['ticker'].str.match(r'^[A-Z\\.]+$')\n",
    "news = news[is_valid.fillna(False)]\n",
    "\n",
    "# Start with tickers that have enough coverage for ML dev\n",
    "min_days = 200\n",
    "ticker_counts = news['ticker'].value_counts()\n",
    "tickers = ticker_counts.loc[ticker_counts >= min_days].index.tolist()\n",
    "\n",
    "len(tickers), tickers[:10]\n",
    "len(tickers), tickers[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dcb581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[BS11052025] fp_610_000030\\n[BS11052025] filter the sentiment analysis down to just the tickers selected in the last step\\n    and then find the earliest date and most recent date\\n    NB: For testing purposes\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "[BS11072025] fp_610_000030\n",
    "[BS11072025] filter the sentiment analysis down to just the tickers selected in the last step\n",
    "    and then find the earliest date and most recent date\n",
    "    NB: For testing purposes\n",
    "'''\n",
    "#news_filtered = news[news['ticker'].isin(tickers)].copy()\n",
    "\n",
    "#news_filtered['date'].min(), news_filtered['date'].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f11e8fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found existing valid ticker file: /workspace/data/valid_tickers.csv\n",
      "Loaded 2644 valid tickers.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "[BS11072025] fp_610_000030\n",
    "[BS11072025] yFinance does not contain info for most delisted stocks. \n",
    "             Of the stocks we have enough data from in the past 10 years, we need to remove\n",
    "             all the delisted stocks. This will ping yFinance to check if the ticker exists\n",
    "             and create a list of delisted stocks that we need to remove from our data.\n",
    "             This took 18 minutes to run so save off the file if it doesn't already exist, otherwise skip.\n",
    "'''\n",
    "GLOBAL_START = \"2014-01-01\"\n",
    "GLOBAL_END   = \"2024-01-09\"\n",
    "\n",
    "def has_prices_in_window(ticker, start=GLOBAL_START, end=GLOBAL_END, max_retries=2):\n",
    "    \"\"\"Return True if YF returns any rows for (ticker, start..end).\"\"\"\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            df = yf.download(\n",
    "                ticker,\n",
    "                start=start,\n",
    "                end=end,\n",
    "                auto_adjust=True,\n",
    "                threads=False,\n",
    "                progress=False,\n",
    "            )\n",
    "            return not df.empty\n",
    "        except Exception:\n",
    "            time.sleep(2 * attempt)   # small exponential backoff\n",
    "    return False\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# ‚úÖ IF THE FILE EXISTS ‚Üí LOAD AND SKIP PROCESSING\n",
    "# -------------------------------------------------------------------\n",
    "if os.path.exists(file_val_tick):\n",
    "    print(f\"‚úÖ Found existing valid ticker file: {file_val_tick}\")\n",
    "    valid_tickers = pd.read_csv(file_val_tick)[\"ticker\"].tolist()\n",
    "    print(f\"Loaded {len(valid_tickers)} valid tickers.\")\n",
    "    \n",
    "else:\n",
    "    # -------------------------------------------------------------------\n",
    "    # ‚ùó FILE DOES NOT EXIST ‚Üí RUN FULL PROBING + SAVE RESULTS\n",
    "    # -------------------------------------------------------------------\n",
    "    print(\"üîç No valid ticker file found. Running yfinance validation step...\")\n",
    "    \n",
    "    valid_tickers = []\n",
    "    invalid_tickers = []\n",
    "\n",
    "    for t in tickers:      # 'tickers' is your filtered high-coverage list\n",
    "        ok = has_prices_in_window(t)\n",
    "        (valid_tickers if ok else invalid_tickers).append(t)\n",
    "\n",
    "    print(f\"‚úÖ Valid: {len(valid_tickers)}  ‚ùå Invalid: {len(invalid_tickers)}\")\n",
    "\n",
    "    # Save the valid ones for next time\n",
    "    pd.Series(valid_tickers, name=\"ticker\").to_csv(file_val_tick, index=False)\n",
    "    print(f\"üíæ Saved valid tickers to {file_val_tick}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45224310",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[BS11072025] fp_610_000035\n",
    "[BS11072025] Load valid tickers and narrow sentiment down to just valid tickers\n",
    "'''\n",
    "# Load validated tickers  \n",
    "valid_tickers = pd.read_csv(file_val_tick)[\"ticker\"].tolist()\n",
    "\n",
    "# Keep only sentiment rows with validated tickers\n",
    "news = news[news['ticker'].isin(valid_tickers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2c3c739",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[BS11072025] fp_610_000040\n",
    "[BS11072025] Get the earliest and latest sentiment date per ticker\n",
    "    NB: I pad the days by 7 weeks on both ends for computing next-day returns\n",
    "'''\n",
    "win = (\n",
    "    news.groupby(\"ticker\")[\"date\"]\n",
    "        .agg(min_date='min', max_date='max')\n",
    "        .reset_index()\n",
    ")\n",
    "\n",
    "pad = pd.Timedelta(days=7)\n",
    "win[\"start\"] = pd.to_datetime(win[\"min_date\"]) - pad\n",
    "win[\"end\"]   = pd.to_datetime(win[\"max_date\"]) + pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78293809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found existing prices file: C:\\Users\\Brad\\Documents\\MSML610 project\\Project data\\ticker_prices.parquet\n",
      "Loaded prices: 2644 tickers, 6,651,728 rows\n"
     ]
    }
   ],
   "source": [
    "#### THIS IS AS FAR AS I MADE IT TO CHANGES SO THAT IT WAS IN THE FORMAT REQUIRED ####\n",
    "#### I WAS USING ANACONDA/VSCode BEFORE THE PR WAS DUE ####\n",
    "\n",
    "'''\n",
    "[BS11072025] fp_610_000045\n",
    "[BS11072025] If the file_prices data has not been saved off\n",
    "             1) Pull the price data per valid ticker for the times done in step fp_610_000040\n",
    "             2) put it all together and save it off to file_prices\n",
    "'''\n",
    "# ----------------------------------------------------------\n",
    "# ‚úÖ If file exists ‚Üí load & skip downloading\n",
    "# ----------------------------------------------------------\n",
    "if os.path.exists(file_prices):\n",
    "    print(f\"‚úÖ Found existing prices file: {file_prices}\")\n",
    "    prices = pd.read_parquet(file_prices)\n",
    "    print(f\"Loaded prices: {prices['ticker'].nunique()} tickers, {len(prices):,} rows\")\n",
    "    \n",
    "else:\n",
    "    # ------------------------------------------------------\n",
    "    # ‚ùó File does NOT exist ‚Üí run full download\n",
    "    # ------------------------------------------------------\n",
    "    print(\"üîç No saved prices found ‚Äî downloading from yfinance...\")\n",
    "\n",
    "    BATCH_SIZE = 25\n",
    "    MAX_RETRIES = 4\n",
    "    BASE_SLEEP = 8\n",
    "\n",
    "    def chunked(lst, n):\n",
    "        for i in range(0, len(lst), n):\n",
    "            yield lst[i:i+n]\n",
    "\n",
    "    frames = []\n",
    "    failed_soft = []\n",
    "    failed_hard = []\n",
    "    downloaded_ok = set()\n",
    "\n",
    "    for batch_idx, batch in enumerate(chunked(valid_tickers, BATCH_SIZE), start=1):\n",
    "\n",
    "        # Per-batch date ranges\n",
    "        sub = win[win[\"ticker\"].isin(batch)]\n",
    "        start = sub[\"start\"].min().strftime(\"%Y-%m-%d\")\n",
    "        end   = sub[\"end\"].max().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        tries = 0\n",
    "        while True:\n",
    "            try:\n",
    "                print(f\"[Batch {batch_idx}] Downloading {len(batch)} tickers...\")\n",
    "                px = yf.download(\n",
    "                    batch,\n",
    "                    start=start,\n",
    "                    end=end,\n",
    "                    auto_adjust=True,\n",
    "                    group_by=\"ticker\",\n",
    "                    progress=False,\n",
    "                    threads=False\n",
    "                )\n",
    "\n",
    "                # Determine which tickers we got data for\n",
    "                if isinstance(px.columns, pd.MultiIndex):\n",
    "                    got = set(px.columns.get_level_values(0))\n",
    "                else:\n",
    "                    got = set(batch) if not px.empty else set()\n",
    "\n",
    "                # For each ticker successfully returned, flatten & save\n",
    "                if not px.empty and isinstance(px.columns, pd.MultiIndex):\n",
    "                    for t in got:\n",
    "                        p = px[t].reset_index().rename(columns={\"Date\": \"date\"})\n",
    "                        p[\"ticker\"] = t\n",
    "                        p[\"date\"] = pd.to_datetime(p[\"date\"]).dt.date\n",
    "                        frames.append(p)\n",
    "                        downloaded_ok.add(t)\n",
    "\n",
    "                # Missing tickers\n",
    "                missing = [t for t in batch if t not in got]\n",
    "                for t in missing:\n",
    "                    failed_hard.append((t, \"no_price_data_in_window\"))\n",
    "\n",
    "                break  # batch done\n",
    "\n",
    "            except Exception as e:\n",
    "                tries += 1\n",
    "                if tries <= MAX_RETRIES:\n",
    "                    sleep_s = BASE_SLEEP * (2 ** (tries - 1))\n",
    "                    print(f\"   Error: {e} ‚Äî retrying in {sleep_s:.0f}s...\")\n",
    "                    time.sleep(sleep_s)\n",
    "                else:\n",
    "                    failed_soft.append((batch, f\"batch_error:{repr(e)}\"))\n",
    "                    break\n",
    "\n",
    "    print(\"‚úÖ Finished downloading price data\")\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # ‚úÖ Assemble `prices` DataFrame\n",
    "    # ------------------------------------------------------\n",
    "    prices = (\n",
    "        pd.concat(frames, ignore_index=True)\n",
    "          .rename(columns={\n",
    "              \"Open\":\"open\",\"High\":\"high\",\"Low\":\"low\",\n",
    "              \"Close\":\"close\",\"Adj Close\":\"adj_close\",\"Volume\":\"volume\"\n",
    "          })\n",
    "    )\n",
    "    if \"adj_close\" not in prices.columns:\n",
    "        prices[\"adj_close\"] = prices[\"close\"]\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # ‚úÖ Save for future use\n",
    "    # ------------------------------------------------------\n",
    "    prices.to_parquet(file_prices, index=False)\n",
    "    print(f\"üíæ Saved prices to {file_prices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17831712",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[BS11072025] fp_610_000050\n",
    "[BS11072025] Trim the price data to each ticker's window\n",
    "'''\n",
    "# merge windows\n",
    "temp = win[[\"ticker\", \"min_date\", \"max_date\"]].copy()\n",
    "temp[\"min_date\"] = pd.to_datetime(temp[\"min_date\"]).dt.date\n",
    "temp[\"max_date\"] = pd.to_datetime(temp[\"max_date\"]).dt.date\n",
    "\n",
    "prices = prices.merge(temp, on=\"ticker\", how=\"inner\")\n",
    "prices = prices[\n",
    "    (prices[\"date\"] >= prices[\"min_date\"]) &\n",
    "    (prices[\"date\"] <= prices[\"max_date\"])\n",
    "]\n",
    "prices = prices.drop(columns=[\"min_date\",\"max_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8a70031",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[BS11072025] fp_610_000055\n",
    "[BS11072025] Merge sentiment data and price data, fill 0 for non news days \n",
    "'''\n",
    "df = prices.merge(\n",
    "    news,\n",
    "    on=[\"ticker\", \"date\"],\n",
    "    how=\"left\"\n",
    ").sort_values([\"ticker\", \"date\"])\n",
    "\n",
    "# fill missing sentiment days with 0 indicating it is a news-free day\n",
    "sent_cols = [\"avg_sentiment\",\"pos_count\",\"neg_count\",\"news_count\"]\n",
    "for c in sent_cols:\n",
    "    df[c] = df[c].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fce5a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found existing engineered model file. Loading...\n",
      "Loaded df_model with shape: (6150445, 28)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "[BS11072025] fp_610_000060\n",
    "[BS11072025] Feature enggineer additional flags\n",
    "'''\n",
    "# ‚úÖ If the model file already exists ‚Üí load it and skip all heavy feature engineering\n",
    "if file_model.exists():\n",
    "    print(\"‚úÖ Found existing engineered model file. Loading...\")\n",
    "    df_model = pd.read_parquet(file_model)\n",
    "    print(f\"Loaded df_model with shape: {df_model.shape}\")\n",
    "else:\n",
    "    print(\"üîç No model file found. Running full feature engineering...\")\n",
    "\n",
    "    # 1-day ahead return\n",
    "    df[\"ret_1d\"] = (\n",
    "        df.groupby(\"ticker\")[\"adj_close\"]\n",
    "        .pct_change(fill_method=None)\n",
    "        .shift(-1)\n",
    "    )\n",
    "\n",
    "    # lag features (prevent leakage)\n",
    "    df[\"sent_lag1\"] = df.groupby(\"ticker\")[\"avg_sentiment\"].shift(1)\n",
    "\n",
    "    # example rolling window\n",
    "    df[\"sent_roll3\"] = (\n",
    "        df.groupby(\"ticker\")[\"avg_sentiment\"]\n",
    "        .apply(lambda s: s.shift(1).rolling(3, min_periods=1).mean())\n",
    "        .values\n",
    "    )\n",
    "\n",
    "    # Drop rows without a target\n",
    "    df_model = df.dropna(subset=[\"ret_1d\"]).reset_index(drop=True)\n",
    "\n",
    "    # lagged returns\n",
    "    df = df.sort_values([\"ticker\", \"date\"]).copy()\n",
    "\n",
    "    df[\"ret_1d_past\"] = (\n",
    "        df.groupby(\"ticker\")[\"adj_close\"].pct_change(fill_method=None).shift(1)\n",
    "    )\n",
    "    df[\"ret_5d_past\"] = (\n",
    "        df.groupby(\"ticker\")[\"adj_close\"].pct_change(5, fill_method=None).shift(1)\n",
    "    )\n",
    "\n",
    "    df[\"ret_10d_past\"] = (\n",
    "        df.groupby(\"ticker\")[\"adj_close\"].pct_change(10, fill_method=None).shift(1)\n",
    "    )\n",
    "\n",
    "    # rolling volatility 10 and 20 day\n",
    "    df[\"vol_roll10\"] = (\n",
    "        df.groupby(\"ticker\")[\"adj_close\"]\n",
    "        .apply(lambda s: s.pct_change(fill_method=None).shift(1)\n",
    "                                .rolling(10, min_periods=5).std())\n",
    "        .values\n",
    "    )\n",
    "\n",
    "    df[\"vol_roll20\"] = (\n",
    "        df.groupby(\"ticker\")[\"adj_close\"]\n",
    "        .apply(lambda s: s.pct_change(fill_method=None).shift(1)\n",
    "                                .rolling(20, min_periods=5).std())\n",
    "        .values\n",
    "    )\n",
    "\n",
    "    # rolling sentiment mean per 7 day\n",
    "    df[\"sent_roll7\"] = (\n",
    "        df.groupby(\"ticker\")[\"avg_sentiment\"]\n",
    "        .apply(lambda s: s.shift(1).rolling(7, min_periods=3).mean())\n",
    "        .values\n",
    "    )\n",
    "\n",
    "    # rolling sentiment count per 7 day\n",
    "    df[\"news_count_roll7\"] = (\n",
    "        df.groupby(\"ticker\")[\"news_count\"]\n",
    "        .apply(lambda s: s.shift(1).rolling(7, min_periods=3).sum())\n",
    "        .values\n",
    "    )\n",
    "\n",
    "    # volume based features\n",
    "    df[\"vol_lag1\"] = df.groupby(\"ticker\")[\"volume\"].shift(1)\n",
    "\n",
    "    df[\"vol_roll10\"] = (\n",
    "        df.groupby(\"ticker\")[\"volume\"]\n",
    "        .apply(lambda s: s.shift(1).rolling(10, min_periods=5).mean())\n",
    "        .values\n",
    "    )\n",
    "\n",
    "    df[\"vol_surge\"] = df[\"volume\"] / df[\"vol_roll10\"]\n",
    "\n",
    "    # slop of recent price movement\n",
    "    def rolling_slope(x):\n",
    "        if len(x) < 5:\n",
    "            return np.nan\n",
    "        y = x.values.reshape(-1,1)\n",
    "        X = np.arange(len(x)).reshape(-1,1)\n",
    "        model = LinearRegression().fit(X, y)\n",
    "        return model.coef_[0][0]\n",
    "\n",
    "    df[\"trend_slope_5\"] = (\n",
    "        df.groupby(\"ticker\")[\"adj_close\"]\n",
    "        .apply(lambda s: s.shift(1).rolling(5).apply(rolling_slope, raw=False))\n",
    "        .values\n",
    "    )\n",
    "\n",
    "    # binary sentiment flags\n",
    "    df[\"sent_pos\"] = (df[\"avg_sentiment\"] > 0).astype(int)\n",
    "    df[\"sent_neg\"] = (df[\"avg_sentiment\"] < 0).astype(int)\n",
    "\n",
    "    # overnight return\n",
    "    df[\"overnight_ret\"] = df.groupby(\"ticker\")[\"open\"].shift(-1) / df[\"close\"] - 1\n",
    "\n",
    "    # Final model df\n",
    "    df_model = df.dropna(subset=[\"ret_1d\"]).reset_index(drop=True)\n",
    "\n",
    "    # Save the final engineered model dataset\n",
    "    df_model.to_parquet(file_model, index=False)\n",
    "    print(f\"üíæ Saved engineered model to {file_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bc497c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[BS11072025] fp_610_000065\n",
    "[BS11072025] load df_model\n",
    "'''\n",
    "df_model = pd.read_parquet(file_model)\n",
    "df_model[\"date\"] = pd.to_datetime(df_model[\"date\"]).dt.date  # ensure date type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da24a675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 25 features: ['avg_sentiment', 'pos_count', 'neg_count', 'news_count', 'sent_lag1', 'sent_roll3', 'sent_roll7', 'news_count_roll7', 'sent_pos', 'sent_neg', 'adj_close', 'open', 'high', 'low', 'close', 'volume', 'ret_1d_past', 'ret_5d_past', 'ret_10d_past', 'vol_roll10', 'vol_roll20', 'vol_lag1', 'vol_surge', 'trend_slope_5', 'overnight_ret']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "[BS11072025] fp_610_000070\n",
    "[BS11072025] define target and feature columns and narrow down to only rows with all features present\n",
    "'''\n",
    "# Target\n",
    "y = df_model[\"ret_1d\"].astype(\"float32\")\n",
    "\n",
    "# Candidate features: include what we engineered; take intersection with existing columns\n",
    "candidate_feats = [\n",
    "    # sentiment\n",
    "    \"avg_sentiment\",\"pos_count\",\"neg_count\",\"news_count\",\n",
    "    \"sent_lag1\",\"sent_roll3\",\"sent_roll7\",\"news_count_roll7\",\n",
    "    \"sent_pos\",\"sent_neg\",\n",
    "    # price/volume context\n",
    "    \"adj_close\",\"open\",\"high\",\"low\",\"close\",\"volume\",\n",
    "    \"ret_1d_past\",\"ret_5d_past\",\"ret_10d_past\",\n",
    "    \"vol_roll10\",\"vol_roll20\",\"vol_lag1\",\"vol_surge\",\n",
    "    \"trend_slope_5\",\"overnight_ret\",\n",
    "]\n",
    "feat_cols = [c for c in candidate_feats if c in df_model.columns]\n",
    "\n",
    "# Keep only rows with all features present\n",
    "X = df_model[[\"ticker\",\"date\"] + feat_cols].copy()\n",
    "mask = X[feat_cols].notna().all(axis=1) & y.notna()\n",
    "X, y = X.loc[mask].reset_index(drop=True), y.loc[mask].reset_index(drop=True)\n",
    "\n",
    "print(f\"Using {len(feat_cols)} features:\", feat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66d9adde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train span: 2014-01-08 ‚Üí 2022-12-30 (5,465,298 rows)\n",
      "Test  span: 2023-01-03 ‚Üí 2024-01-08 (482,433 rows)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "[BS11072025] fp_610_000075\n",
    "[BS11072025] Do train/test split\n",
    "'''\n",
    "# Data goes up to 2024-01-09 so use approximately last 12 months of data as test\n",
    "cutoff = pd.to_datetime(\"2023-01-01\").date()\n",
    "train_idx = X[\"date\"] <  cutoff\n",
    "test_idx  = X[\"date\"] >= cutoff\n",
    "\n",
    "X_train, X_test = X.loc[train_idx], X.loc[test_idx]\n",
    "y_train, y_test = y.loc[train_idx], y.loc[test_idx]\n",
    "\n",
    "print(\"Train span:\", X_train[\"date\"].min(), \"‚Üí\", X_train[\"date\"].max(), f\"({len(X_train):,} rows)\")\n",
    "print(\"Test  span:\", X_test[\"date\"].min(), \"‚Üí\", X_test[\"date\"].max(),  f\"({len(X_test):,} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7d68e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[BS11072025] fp_610_000080\n",
    "[BS11072025] Preprocess data by scaling numerics and using one-hot encoding for ticker\n",
    "'''\n",
    "num_features = feat_cols\n",
    "cat_features = [\"ticker\"]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), cat_features),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6b844d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows after top-N tickers: 554695\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "[BS11072025] fp_610_000085\n",
    "[BS11072025] Limit model \n",
    "            NB: I've reduced this to the top 200 tickers for testing purposes\n",
    "                since I will likely need to offload this computation to a different computer\n",
    "                with higher RAM\n",
    "'''\n",
    "top_n = 200\n",
    "top_tickers = (X_train.groupby(\"ticker\", as_index=False)\n",
    "                        .size()\n",
    "                        .sort_values(\"size\", ascending=False)\n",
    "                        .head(top_n)[\"ticker\"])\n",
    "\n",
    "keep = X_train[\"ticker\"].isin(top_tickers)\n",
    "X_train, y_train = X_train.loc[keep].reset_index(drop=True), y_train.loc[keep].reset_index(drop=True)\n",
    "print(\"Train rows after top-N tickers:\", len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec7fd945",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[BS11072025] fp_610_000090\n",
    "[BS11072025] Limit model \n",
    "            NB: I've reduced the number of features and downcast them for testing purposes\n",
    "            since I will likely need to offload this computation to a different computer\n",
    "            with higher RAM\n",
    "'''\n",
    "# Start lean: drop the most memory/CPU-heavy features\n",
    "drop_feats = {\"trend_slope_5\", \"vol_roll20\", \"vol_roll10\", \"vol_surge\"}\n",
    "num_features = [c for c in feat_cols if c not in drop_feats]\n",
    "cat_features = [\"ticker\"]\n",
    "\n",
    "for c in num_features:\n",
    "    X_train[c] = X_train[c].astype(\"float32\", copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e643c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned X_train shape: (554695, 27)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "[BS11072025] fp_610_000095\n",
    "[BS11072025] Remove any infinite of null values\n",
    "'''\n",
    "# Clean X_train: remove inf and NaN\n",
    "X_train = X_train.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "mask = X_train.notna().all(axis=1) & y_train.notna()\n",
    "X_train = X_train.loc[mask].reset_index(drop=True)\n",
    "y_train = y_train.loc[mask].reset_index(drop=True)\n",
    "\n",
    "print(\"‚úÖ Cleaned X_train shape:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30dc9557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'inproc://192.168.68.68/10716/1' processes=1 threads=1, memory=2.79 GiB>\n",
      "dict_keys(['inproc://192.168.68.68/10716/4'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-07 18:20:36,337 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.10 GiB -- Worker memory limit: 2.79 GiB\n",
      "c:\\Users\\Brad\\anaconda3\\envs\\MSML610_FIN_PROJ\\Lib\\site-packages\\tpot\\tpot_estimator\\estimator.py:458: UserWarning: Both generations and max_time_mins are set. TPOT will terminate when the first condition is met.\n",
      "  warnings.warn(\"Both generations and max_time_mins are set. TPOT will terminate when the first condition is met.\")\n",
      "Generation:   0%|          | 0/8 [00:00<?, ?it/s]2025-11-07 18:20:51,807 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.24 GiB -- Worker memory limit: 2.79 GiB\n",
      "2025-11-07 18:21:01,915 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 2.22 GiB -- Worker memory limit: 2.79 GiB\n",
      "2025-11-07 18:21:03,635 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.24 GiB -- Worker memory limit: 2.79 GiB\n",
      "2025-11-07 18:21:10,065 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.18 GiB -- Worker memory limit: 2.79 GiB\n",
      "Generation:  12%|‚ñà‚ñé        | 1/8 [1:00:04<7:00:32, 3604.70s/it]"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "No individuals could be evaluated in the initial population as the max_eval_mins time limit was reached before any individuals could be evaluated.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     19\u001b[39m tpot = TPOTRegressor(\n\u001b[32m     20\u001b[39m     generations=\u001b[32m8\u001b[39m,\n\u001b[32m     21\u001b[39m     population_size=\u001b[32m20\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     31\u001b[39m     memory_limit=\u001b[33m\"\u001b[39m\u001b[33m3GB\u001b[39m\u001b[33m\"\u001b[39m,        \u001b[38;5;66;03m# harmless duplicate; TPOT uses this when it creates clusters\u001b[39;00m\n\u001b[32m     32\u001b[39m )\n\u001b[32m     34\u001b[39m pipe = Pipeline([(\u001b[33m\"\u001b[39m\u001b[33mprep\u001b[39m\u001b[33m\"\u001b[39m, preprocess), (\u001b[33m\"\u001b[39m\u001b[33mtpot\u001b[39m\u001b[33m\"\u001b[39m, tpot)])\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Brad\\anaconda3\\envs\\MSML610_FIN_PROJ\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Brad\\anaconda3\\envs\\MSML610_FIN_PROJ\\Lib\\site-packages\\sklearn\\pipeline.py:663\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    657\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    658\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    659\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    660\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    661\u001b[39m             all_params=params,\n\u001b[32m    662\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Brad\\anaconda3\\envs\\MSML610_FIN_PROJ\\Lib\\site-packages\\tpot\\tpot_estimator\\templates\\tpottemplates.py:305\u001b[39m, in \u001b[36mTPOTRegressor.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    277\u001b[39m     \u001b[38;5;28msuper\u001b[39m(TPOTRegressor,\u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m(\n\u001b[32m    278\u001b[39m         search_space=search_space,\n\u001b[32m    279\u001b[39m         scorers=\u001b[38;5;28mself\u001b[39m.scorers, \n\u001b[32m   (...)\u001b[39m\u001b[32m    301\u001b[39m         random_state=\u001b[38;5;28mself\u001b[39m.random_state,\n\u001b[32m    302\u001b[39m         **\u001b[38;5;28mself\u001b[39m.tpotestimator_kwargs)\n\u001b[32m    303\u001b[39m     \u001b[38;5;28mself\u001b[39m.initialized = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Brad\\anaconda3\\envs\\MSML610_FIN_PROJ\\Lib\\site-packages\\tpot\\tpot_estimator\\estimator.py:766\u001b[39m, in \u001b[36mTPOTEstimator.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m(\u001b[38;5;28mself\u001b[39m.warm_start \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._evolver_instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    720\u001b[39m     \u001b[38;5;28mself\u001b[39m._evolver_instance = \u001b[38;5;28mself\u001b[39m._evolver(   individual_generator=ind_generator(\u001b[38;5;28mself\u001b[39m.rng),\n\u001b[32m    721\u001b[39m                                     objective_functions= [objective_function],\n\u001b[32m    722\u001b[39m                                     objective_function_weights = \u001b[38;5;28mself\u001b[39m.objective_function_weights,\n\u001b[32m   (...)\u001b[39m\u001b[32m    762\u001b[39m                                     rng=\u001b[38;5;28mself\u001b[39m.rng,\n\u001b[32m    763\u001b[39m                                     )\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evolver_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[38;5;66;03m#self._evolver_instance.population.update_pareto_fronts(self.objective_names, self.objective_function_weights)\u001b[39;00m\n\u001b[32m    768\u001b[39m \u001b[38;5;28mself\u001b[39m.make_evaluated_individuals()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Brad\\anaconda3\\envs\\MSML610_FIN_PROJ\\Lib\\site-packages\\tpot\\evolvers\\base_evolver.py:513\u001b[39m, in \u001b[36mBaseEvolver.optimize\u001b[39m\u001b[34m(self, generations)\u001b[39m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m time.time() - start_time > \u001b[38;5;28mself\u001b[39m.max_time_mins*\u001b[32m60\u001b[39m:\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.population.evaluated_individuals[\u001b[38;5;28mself\u001b[39m.objective_names].isnull().all().iloc[\u001b[32m0\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m513\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo individuals could be evaluated in the initial population as the max_eval_mins time limit was reached before any individuals could be evaluated.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    514\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    515\u001b[39m \u001b[38;5;28mself\u001b[39m.step()\n",
      "\u001b[31mException\u001b[39m: No individuals could be evaluated in the initial population as the max_eval_mins time limit was reached before any individuals could be evaluated."
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) Tiny, thread-only cluster to avoid Windows nanny/process issues\n",
    "cluster = LocalCluster(\n",
    "    n_workers=1,             # keep it simple; scale later if needed\n",
    "    threads_per_worker=1,    # predictable memory use\n",
    "    processes=False,         # IMPORTANT: no separate processes ‚áí no nanny\n",
    "    memory_limit=\"3GB\",      # leave headroom for the OS\n",
    "    dashboard_address=None,  # don‚Äôt open a port\n",
    ")\n",
    "client = Client(cluster)\n",
    "\n",
    "# (Optional) quick sanity check\n",
    "print(client)\n",
    "print(client.scheduler_info()[\"workers\"].keys())\n",
    "\n",
    "# 2) Time-aware CV as before\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# 3) TPOT configured for Dask client you created\n",
    "tpot = TPOTRegressor(\n",
    "    generations=8,\n",
    "    population_size=20,\n",
    "    cv=tscv,\n",
    "    scorers=[\"neg_root_mean_squared_error\"],  # TPOT 0.12+ expects scorer names\n",
    "    bigger_is_better=True,                    # maximize neg RMSE = minimize RMSE\n",
    "    max_eval_time_mins=5,\n",
    "    n_jobs=1,                  # let Dask control parallelism (we‚Äôre 1 worker anyway)\n",
    "    processes=False,           # bool, not int\n",
    "    verbose=2,\n",
    "    client=client,             # <-- give TPOT the client so it won't create its own\n",
    "    scatter=True,\n",
    "    memory_limit=\"3GB\",        # harmless duplicate; TPOT uses this when it creates clusters\n",
    ")\n",
    "\n",
    "pipe = Pipeline([(\"prep\", preprocess), (\"tpot\", tpot)])\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb49ac2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
