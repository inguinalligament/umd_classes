// notes_to_pdf.py --input msml610/lectures_source/figures/Lesson5-Theory_Statistical_learning.txt --output tmp.pdf --type slides --debug_on_error --skip_action cleanup_after --toc_type navigation

// /Users/saggese/Library/CloudStorage/GoogleDrive-saggese@gmail.com/My\ Drive/books/Math\ -\ Machine\ learning/LearningFromData/Abu-Mostafa\ Yaser\ S.,\ Malik\ Magdon\ \(2012\)\ --Ismail,\ et\ al.,\ Learning\ From\ Data\ -\ A\ short\ course\ \(2012\).pdf 

::: columns
:::: {.column width=15%}
![](msml610/lectures_source/figures/UMD_Logo.png)
::::
:::: {.column width=75%}

\vspace{0.4cm}
\begingroup \large
MSML610: Advanced Machine Learning
\endgroup
::::
:::

\vspace{1cm}

\begingroup \Large
**$$\text{\blue{Machine Learning Theories}}$$**
\endgroup
\vspace{1cm}

::: columns
:::: {.column width=75%}
\vspace{1cm}
**Instructor**: Dr. GP Saggese - `gsaggese@umd.edu`

**References**:

- Abu-Mostafa et al.: _"Learning From Data"_ (2012)

::::
:::: {.column width=20%}
![](msml610/lectures_source/figures/book_covers/Book_cover_Learning_from_Data.jpg){ height=20% }
::::
:::

# Is Machine Learning Even Possible?

* A Simple Visual ML Experiment (1/2)

::: columns
:::: {.column width=50%}
- Consider the supervised classification problem

- **\black{Input}**
  - A 9 bit vector represented as a 3x3 array
- **\black{Training set}**
  - The \blue{blue row} $\vx_1, \vx_2, \vx_3$ for $f(\vx) = -1$
  - The \green{green row} $\vx_4, \vx_5, \vx_6$ for $f(\vx) = +1$
- **\black{Test set}**
  - For the \red{red pattern} $\vx_0$, is $f(\vx_0) = -1 \text{ or } +1$?

::::
:::: {.column width=50%}

```latex
\usepackage{tikz}
\begin{document}

\newcommand{\gridpattern}[2]{
  \begin{tikzpicture}[scale=0.4]
    \foreach \x in {0,...,2}{
      \foreach \y in {0,...,2}{
        \pgfmathsetmacro{\v}{#1[\y*3+\x]}
        \draw[black] (\x,-\y) rectangle ++(1,-1); % draw grid cell
        \ifnum \v=1
          \fill[#2] (\x+0.1,-\y-0.1) rectangle ++(0.8,-0.8); % slightly smaller fill
        \fi
      }
    }
  \end{tikzpicture}
}

%\begin{center}
\begin{tikzpicture}
  \matrix[row sep=1em] {
    \node{\gridpattern{{1,0,0,1,0,1,0,1,0}}{blue}}; &
    \node{\gridpattern{{1,0,0,0,0,1,1,0,1}}{blue}}; &
    \node{\gridpattern{{1,0,0,0,0,1,0,0,0}}{blue}}; &
    \node{\(f = -1\)}; \\
    \node{\gridpattern{{0,0,1,0,1,0,1,0,0}}{green}}; &
    \node{\gridpattern{{0,1,0,1,0,1,0,1,0}}{green}}; &
    \node{\gridpattern{{0,1,1,1,1,0,0,1,1}}{green}}; &
    \node{\(f = +1\)}; \\
  };
  \node at (-0.6,-3.0) {\gridpattern{{1,0,0,0,1,0,0,0,1}}{red}};
  \node at (1.4,-3.0) {\(f = ?\)};
\end{tikzpicture}
%\end{center}
\end{document}
```
::::
:::

* A Simple Visual ML Experiment (2/2)

::: columns
:::: {.column width=70%}

- **\black{Model 1}**
  - $f(\vx) = +1$ when $\vx$ has an axis of symmetry
  - $f(\vx) = -1$ when $\vx$ is not symmetric
  - The test set is symmetrical $\implies f(\vx_0) = +1$

- **\black{Model 2}**
  - $f(\vx) = +1$ when the top left square $\vx$ is empty
  - $f(\vx) = -1$ when the top left square $\vx$ is full
  - The test set has top left square full $\implies f(\vx_0) = -1$

- Many functions fit the 6 training examples
  - Some have a value of -1 on the test point, others +1
  - Which one is it?

- How can a limited data set reveal enough information to define the entire
  target function?
  - **\black{Is machine learning possible?}**
::::
:::: {.column width=30%}

```latex
\usepackage{tikz}
\begin{document}

\newcommand{\gridpattern}[2]{
  \begin{tikzpicture}[scale=0.4]
    \foreach \x in {0,...,2}{
      \foreach \y in {0,...,2}{
        \pgfmathsetmacro{\v}{#1[\y*3+\x]}
        \draw[black] (\x,-\y) rectangle ++(1,-1); % draw grid cell
        \ifnum \v=1
          \fill[#2] (\x+0.1,-\y-0.1) rectangle ++(0.8,-0.8); % slightly smaller fill
        \fi
      }
    }
  \end{tikzpicture}
}

%\begin{center}
\begin{tikzpicture}
  \matrix[row sep=1em] {
    \node{\gridpattern{{1,0,0,1,0,1,0,1,0}}{blue}}; &
    \node{\gridpattern{{1,0,0,0,0,1,1,0,1}}{blue}}; &
    \node{\gridpattern{{1,0,0,0,0,1,0,0,0}}{blue}}; &
    \node{\(f = -1\)}; \\
    \node{\gridpattern{{0,0,1,0,1,0,1,0,0}}{green}}; &
    \node{\gridpattern{{0,1,0,1,0,1,0,1,0}}{green}}; &
    \node{\gridpattern{{0,1,1,1,1,0,0,1,1}}{green}}; &
    \node{\(f = +1\)}; \\
  };
  \node at (-0.6,-3.0) {\gridpattern{{1,0,0,0,1,0,0,0,1}}{red}};
  \node at (1.4,-3.0) {\(f = ?\)};
\end{tikzpicture}
%\end{center}
\end{document}
```

::::
:::

* Is Machine Learning Possible?
- The function can assume **any value outside data**
  - E.g., with summer temperature data, the function could assume a different
    value for winter

- **How to learn an unknown function?**
  - Estimating at unseen points seems impossible in general
  - Requires assumptions or models about behavior

- Difference between:
  - **Possible**
    - No knowledge of the unknown function
    - E.g., could be linear, quadratic, or sine wave outside known data
  - **Probable**
    - Some knowledge of the unknown function from domain knowledge or historical
      data patterns
    - E.g., if historical weather data forms a sinusoidal pattern, unknown
      points likely follow that pattern

* Supervised Learning: Bin Analogy (1/2)

::: columns
:::: {.column width=50%}

- Consider a bin with \red{red} and \green{green} marbles
  - We want to estimate $\Pr(\text{pick a \red{red} marble}) = \mu$ where the
    value of $\mu$ is unknown
  - We pick $N$ marbles independently with replacement
  - The fraction of \red{red} marbles is $\nu$

::::
:::: {.column width=45%}

![](msml610/lectures_source/figures/Lesson5_Bin_with_marbles.png)
::::
:::

- Does $\nu$ say anything about $\mu$?
  - **"No"**
    - In strict terms, we don't know anything about the marbles we didn't pick
    - The sample can be mostly \green{green}, while the bin is mostly \red{red}
    - This is _possible_, but _not probable_
  - **"Yes"**
    - Under certain conditions, the sample frequency is close to the real
      frequency

- **Possible vs probable**
  - It is **possible** that we don't know anything about the marbles in the bin
  - It is **probable** that we know something
  - Hoeffding inequality makes this intuition formal

* Hoeffding Inequality
- Consider a Bernoulli random variable $X$ with probability of success $\mu$

- Estimate the mean $\mu$ using $N$ samples with $\nu = \frac{1}{N} \sum_i X_i$

- The **probably approximately correct** (PAC) statement holds:
  $$
  \Pr(|\nu - \mu| > \varepsilon) \le \frac{2}{e^{2 \varepsilon^2 N}}
  $$

- **Remarks:**
  - Valid for all $N$ and $\varepsilon$, not an asymptotic result
  - Holds only if you sample $\nu$ and $\mu$ at random and in the same way
  - If $N$ increases, it is exponentially small that $\nu$ will deviate from
    $\mu$ by more than $\varepsilon$
  - The bound does not depend on $\mu$
  - Trade-off between $N$, $\varepsilon$, and the bound:
    - Smaller $\varepsilon$ requires larger $N$ for the same probability bound
    - Since $\nu \in [\mu - \varepsilon, \mu + \varepsilon]$, you want small
      $\varepsilon$ with a large probability
  - It is a statement about $\nu$ and not $\mu$ although you use it to state
    something about $\nu$ (like for a confidence interval)

* Supervised Learning: Bin Analogy (2/2)

- Let's connect the bin analogy, Hoeffding inequality, and feasibility of
  machine learning
  - You know $f(\vx)$ at points $\vx \in \calX$
  - You choose an hypothesis $h: \calX \rightarrow \calY = \{0, 1\}$
  - Each point $\vx \in \calX$ is a marble
  - You color \red{red} if the hypothesis is correct $h(\vx) = f(\vx)$,
    \green{green} otherwise
  - The in-sample error $E_{in}(h)$ corresponds to $\nu$
  - The marbles of unknown color corresponds to $E_{out}(h) = \mu$
  - $\vx_1, ..., \vx_n$ are picked randomly and independently from a
    distribution over $\calX$ which is the same as for $E_{out}$

- Hoeffding inequality holds and bounds the error going from in-sample to
  out-of-sample
  $$
  \Pr(|E_{in} - E_{out}| > \varepsilon) \le c
  $$
  - Generalization over unknown points (i.e., marbles) is possible
  - **Machine learning is possible!**

* Validation vs Learning: Bin Analogy

- You have learned that for a given $h$, in-sample performance $E_{in}(h) = \nu$
  needs to be close to out-of-sample performance $E_{out}(h) = \mu$
  - This is the **validation setup**, after you have already learned a model

- In a **learning setup** you have $h$ to choose from $M$ hypotheses
  - You need a bound on the out-of-sample performance of the chosen hypothesis
    $h \in \calH$, regardless of which hypothesis you choose
  - You need a Hoeffding counterpart for the case of choosing from multiple
    hypotheses
    \begingroup \small
    \begin{alignat*}{2}
    & \forall g \in \calH = \{h_1, ... , h_M\} \; \Pr(|E_{in}(g) - E_{out}(g)| > \varepsilon)
    &
    \\
    & \hspace{1cm} \le \Pr(\bigcup_{i=1}^M (|E_{in}(h_i) - E_{out}(h_i) | > \varepsilon))
    &
    \\
    & \hspace{1cm} \le \sum_{i=1}^M \Pr(|E_{in}(h_i) - E_{out}(h_i)| > \varepsilon)
    & \text{  (by the union bound)}
    \\
    & \hspace{1cm} \le 2 M \exp(-2 \varepsilon^2 N)
    & \text{  (by Hoeffding)}
    \\
    \end{alignat*}
    \endgroup
- **Problem**: the bound is weak

* Validation vs Learning: Coin Analogy
- In a **validation set-up**, you have a coin and want to determine if it is fair

- Assume the coin is unbiased: $\mu = 0.5$
- Toss the coin 10 times
- How likely is that you get 10 heads (i.e., the coin looks biased $\nu = 0$)?
  $$
  \Pr(\text{coin shows } \nu = 0) = 1 / 2^{10} = 1 / 1024 \approx 0.1\%
  $$

- **Conclusion**: the probability that the out-of-sample performance ($\nu=0.0$)
  is completely different from the in-sample perf ($\mu=0.5$) is very low

* Validation vs Learning: Coin Analogy

- In a **learning set-up**, you have many coins and you need to choose one and
  determine if it's fair

- If you have 1000 fair coins, how likely is it that at least one appears totally
  biased using 10 experiments?
  - I.e., out-of-sample performance is completely different from in-sample
    performance
  $$
  \begin{aligned}
  \Pr(\text{at least one coin has } \nu = 0) &
  = 1 - \Pr(\text{all coins have } \nu \neq 0)\\
  &= 1 - (\Pr(\text{a coin has } \nu \neq 0)) ^{10}\\
  &= 1 - (1 - \Pr(\text{a coin has } \nu = 0)) ^{10}\\
  &= 1 - (1 - 1 / 2 ^ {10}) ^ {1000}\\
  &\approx 0.63\%
  \end{aligned}
  $$

- **Conclusion**: It is probable, more than 50\%

// TODO(gp): Merge the next two slides

* Validation vs Learning: Hoeffding Inequality
- In **validation / testing**
  - Use Hoeffding to assess how well our $g$ (the _chosen hypothesis_)
    approximates $f$ (the _unknown hypothesis_):
    $$
    \Pr(|E_{in} - E_{out}| > \varepsilon) \le 2 \exp(-2 \varepsilon^2 N)
    $$
    where:
    \begingroup \small
    \begin{alignat*}{2}
    & E_{in}(g) = \frac{1}{N} \sum_i e(g(\vx_i), f(\vx_i)) \\
    & E_{out}(g) = \EE_{\vx}[e(g(\vx), f(\vx))] \\
    \end{alignat*}
    \endgroup
  - Since the hypothesis $g$ is final and fixed, Hoeffding inequality guarantees
    that you can learn since it gives a bound for $E_{out}$ to track $E_{in}$

- In **learning**
  - Need to account that our hypothesis is the best of $M$ hypotheses, so:
    $$
    \Pr(|E_{in} - E_{out}| > \varepsilon) \le 2 M \exp(-2 \varepsilon^2 N)
    $$
  - The bound for $E_{out}$ from Hoeffding is weak

- **Questions**:
  - Is the bound weak because it needs to be?
  - Is it possible to replace it with a stricter bound?

* Intuition Why Bound for Hoeffding Is Weak
- The Hoeffding inequality and the union bound applied to training set
  $$
  \Pr(|E_{in} - E_{out}| > \varepsilon) \le 2 M \exp(-2 \varepsilon^2 N)
  $$
  is **artificially** too loose

- $M$ was coming from the bad event:
  \begin{alignat*}{2}
  \calB_i
  &= \textit{"hypothesis $h_i$ does not generalize out-of-sample"} \\
  &= "|E_{in}(h_i) - E_{out}(h_i)| > \varepsilon"
  \end{alignat*}

::: columns
:::: {.column width=65%}
- Since $g \in \{h_1, h_2, \cdots, h_M\}$ then
  $\Pr(\calB)
  \le \Pr(\bigcup_i \calB_i)
  \le \sum_i \Pr(\calB_i)$

- The union bound assumes the events are disjoint, leading to a conservative
  estimate if events overlap
- **In reality**, bad events are extremely overlapping because bad hypotheses are
  extremely similar

::::
:::: {.column width=30%}

```tikz[width=90%]
% Draw the three overlapping colored circles
\draw[thick, red] (0,0) circle(2cm);         % B1
\draw[thick, green] (1,0.5) circle(2cm);     % B2
\draw[thick, blue] (0.5,-1) circle(2cm);     % B3

% Colored labels
\node[text=red] at (-2.3,0) {$\mathcal{B}_1$};
\node[text=green] at (2.2,0.7) {$\mathcal{B}_2$};
\node[text=blue] at (0.3,-2.5) {$\mathcal{B}_3$};
```

::::
:::

//* Why Union Bound for Hoeffding Is Loose: Intuition
//
//// TODO(Gp): Improve this
//
//- Consider two linearly separable classes on a plane, in terms of the ground
//  truth and a training set
//
//- Consider two 2D perceptrons with similar weights: $g_1, g_2$
//- $E_{out}$ is the area where each hypothesis $g_i$ and the ground truth
//  disagree
//- $\Delta E_{out}$ is the differential area between the two $E_{out}$
//
//- $E_{in}$ corresponds to the points in the training set falling in the area
//  corresponding to $E_{out}$
//- $\Delta E_{in}$ is the number of points falling in $\Delta E_{out}$, i.e.,
//  changing classification going from one hypothesis to the other
//
//- Thus the two "bad events" $\calB_1$ and $\calB_2$ are related to
//  $\Delta E_{in}$ and $\Delta E_{out}$

* Training vs Testing: College Course Analogy (1/2)
- In machine learning there are several phases
```graphviz
digraph ML_Phases {
  rankdir=LR;
  node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];

  learning   [label="Learning Phase\n(Training Set)"];
  validation [label="Validation Phase\n(Validation Set)"];
  testing    [label="Testing Phase\n(Test Set)"];
  production [label="Out-of-Sample Phase\n(Production)"];

  learning -> validation -> testing -> production;
}
```

- This set-up is very similar to studying and exams in a college course

- Students study the material
  - This is the **learning phase**

- Before the final exam, students receive practice problems and solutions
  - Studying the problems improves performance by understating what they need to
    improve
  - This corresponds to the **validation set**

* Training vs Testing: College Course Analogy (2/2)

- The final exam corresponds to the **testing phase**
  - These problems are different than the problems in the validation set

  - Why not give out exam problems to improve performance?
    - Doing well in the exam isn't the goal
    - The goal is to learn the course material

  - The final exam isn't strictly necessary
    - Gauges how well you've learned
    - Motivates you to study
    - Knowing exam problems in advance wouldn't gauge learning effectively

- What matters is how students do once they graduate and find a job
  - This is the **out-of-sample phase**

# ##############################################################################
# Growth Function
# ##############################################################################

* Dichotomy: Definition
- **Problem**: classify $N$ (fixed) points $\vx_1, ..., \vx_N$ with an hypothesis
  set $\calH$ of multi-class classifiers
- Consider an assignment $D$ of the points to certain class $\vd_1, ..., \vd_N$
- $D$ is a **dichotomy** for hypothesis set $\calH$ $\iff$ there exists
  $h \in \calH$ that gets the desired classification $D$

::: columns
:::: {.column width=65%}

- **Example**
  - 4 points in a plane $A, B, C, D$
  - Binary classification
  - $\calH$ = \{ bidimensional perceptrons \}
  - Moving the separating hyperplane, you get different classifications for the
    points (i.e., dichotomies)
    ```
          D1    D2   D3   D4   D...
    A     o     x      ...
    B     x     x
    C     o     o
    D     x     o      ...
    ```
  - There are at most $2^N$ dichotomies
  - Certain classifications are not possible (e.g., XOR assignment)

::::
:::: {.column width=30%}

```tikz
% Draw rectangle
\draw[thick] (0,0) rectangle (5,3.5);

% Define coordinates for points
\coordinate (A) at (2.5,3);   % top circle
\coordinate (B) at (3.8,2);   % right cross
\coordinate (C) at (2.5,1);   % bottom circle
\coordinate (D) at (1,1.2);   % left cross

% Draw symbols
\node at (A) {\Large $\circ$};
\node at (B) {\Large $\times$};
\node at (C) {\Large $\circ$};
\node at (D) {\Large $\times$};

% Add labels
\node[above right] at (A) {$A$};
\node[above left] at (B) {$B$};
\node[below right] at (C) {$C$};
\node[below left] at (D) {$D$};

% Define coordinates for points
\coordinate (TopCircle) at (5, 0);
\coordinate (BottomCircle) at (0, 3.5);

% Draw single line between the circles
\draw[red, dotted, thick] (TopCircle) -- (BottomCircle);
```

```tikz
% Draw rectangle
\draw[thick] (0,0) rectangle (5,3.5);

% Define coordinates for points
\coordinate (A) at (2.5,3);   % top circle
\coordinate (B) at (3.8,2);   % right cross
\coordinate (C) at (2.5,1);   % bottom circle
\coordinate (D) at (1,1.2);   % left cross

% Draw symbols
\node at (A) {\Large $\times$};
\node at (B) {\Large $\times$};
\node at (C) {\Large $\circ$};
\node at (D) {\Large $\circ$};

% Add labels
\node[above right] at (A) {$A$};
\node[above left] at (B) {$B$};
\node[below right] at (C) {$C$};
\node[below left] at (D) {$D$};

% Draw single line
\draw[red, dotted, thick] (5, 0) -- (0, 3.5);
```

// TODO: Finish a few plots.

::::
:::

* Dichotomies vs Hypotheses
- An **hypothesis** classifies each point of $\calX$: $\calX \rightarrow \{-1, +1\}$
- A **dichotomy** classifies each point of a fixed set:
  $\{\vx_1, ..., \vx_N\} \rightarrow \{-1, +1\}$
  - Dichotomies are "mini-hypotheses", i.e., hypotheses restricted to given
    points
  - A dichotomy depends on:
    - The number of points $N$
    - Hypothesis set $\calH$ (i.e., the possible models)
    - Where the points are placed
    - How the points are assigned

- The **number of different dichotomies** is indicated by
  $|\calH(\vx_1, ..., \vx_N)|$
  - The number of dichotomies is always finite, since
    $|\calH(\vx_1, ..., \vx_N)| \le N^K$
  - The number of hypotheses is usually infinite, i.e., $|\calH| = \infty$

- The "complexity" of $\calH$ is related to the number of hypothesis

- From the training set point of view what matters are dichotomies and not
  hypotheses
  - Many (infinite) hypotheses can correspond to the same dichotomy

* Growth Function
- The **growth function** counts the maximum number of possible dichotomies on
  $N$ points for a hypothesis set $\calH$:

  $$
  m_{\calH}(N)
  = \max_{\vx_1, \cdots, \vx_N \in \calX} |\calH(\vx_1, \cdots, \vx_N)|
  $$

- **Why growth function?**
  - The dichotomies depend on point distribution and assignment
  - The growth function considers the maximum by placing points in the most
    "favorable way" for the hypothesis set

- To compute $m_{\calH}(N)$ by **brute force**:
  - Consider all possible placements of $N$ points $\vx_1, ..., \vx_N$
  - Consider all possible assignments of the points to the classes
  - Consider all possible hypotheses $h \in \calH$
  - Compute the corresponding dichotomy for $h$ on $\vx_1, ..., \vx_N$
  - Count the number of different dichotomies

* What Can Vary in a Dichotomy
- Given:
  - An hypothesis set $\calH$ (e.g., bidimensional perceptrons)
  - $N$ (fixed) points $\vx_1, ..., \vx_N$
  - An assignment $D$ of the points to certain class $\vd_1, ..., \vd_N$

- $D$ is a **dichotomy** for hypothesis set $\calH$ $\iff$ there exists
  $h \in \calH$ that gets the desired classification $D$

- There are various quantities in the definition of dichotomy
  - The hypothesis set $\calH$
    - It is fixed
  - The number of dimensions of the input space
    - It is fixed through the hypothesis set $\calH$
  - The number of points $N$
    - Input to the growth function $m_{\calH}(N)$
  - How the points are assigned to the classes $\vd_1, ..., \vd_N$
    - It is a free parameter, removed by how each hypothesis in $\calH$ "splits"
      the space 
  - Where the points are positioned $\vx_1, ..., \vx_N$
    - It is a free parameter, removed by the growth function through $\max$

* Growth Function Is Increasing
- $m_{\calH}(N)$ increases (although not monotonically) with $N$
- E.g.,
  - The number of dichotomies on $N=3$ points $m_{\calH}(3)$ is smaller or equal
    than the number of dichotomies on $N=4$ points
  - In fact we can ignore a new point and get the same classification

- $m_{\calH}(N)$ increases with the complexity of $\calH$

- $m_{\calH}(N)$ increases with the number of dimensions in the input space
  (i.e., feature space)

* Growth Function: Examples
- Consider the growth function $m_{\calH}$ for different hypothesis sets $\calH$

::: columns
:::: {.column width=50%}
- **Perceptron on a plane**
  - $m_{\calH}(3) = 8$
  - $m_{\calH}(4) = 14$ (2 XOR classifications not possible)
::::
:::: {.column width=45%}

// TODO(gp): Use x_1, x_2, ... instead of A

```tikz[width=60%]
% Draw rectangle
\draw[thick] (0,0) rectangle (5,3.5);

% Define coordinates for points
\coordinate (A) at (2.5,3);   % top circle
\coordinate (B) at (3.8,2);   % right cross
\coordinate (C) at (2.5,1);   % bottom circle
\coordinate (D) at (1,1.2);   % left cross

% Draw symbols
\node at (A) {\Large $\circ$};
\node at (B) {\Large $\times$};
\node at (C) {\Large $\circ$};
\node at (D) {\Large $\times$};

% Add labels
\node[above right] at (A) {$A$};
\node[above left] at (B) {$B$};
\node[below right] at (C) {$C$};
\node[below left] at (D) {$D$};

% Define coordinates for points
\coordinate (TopCircle) at (5, 0);
\coordinate (BottomCircle) at (0, 3.5);

% Draw single line between the circles
\draw[red, dotted, thick] (TopCircle) -- (BottomCircle);
```

\vspace{1cm}
::::
:::

::: columns
:::: {.column width=50%}
- **Positive rays** $\sign(x - a)$ on $\bbR$
  - $m_{\calH}(N) = N + 1$
  - Origin of rays $a$ can be placed in $N + 1$ intervals

::::
:::: {.column width=45%}

```tikz
    % Draw axis
    \draw[thick,->] (-1,0) -- (8,0) node[right] {};

    % Draw negative samples (crosses)
    \foreach \i in {0, 1, 2, 3} {
        \draw[thick, red] (\i,0) node[below=3pt] {$x_{\the\numexpr\i+1}$} node {\textsf{x}};
    }
    \node at (3.5, -0.3) {$\cdots$};

    % Draw decision boundary
    \draw[thick, dotted, blue] (4.5,-0.3) -- (4.5,1.2) node[above] {$a$};

    % Draw positive samples (circles)
    \foreach \i in {5, 6, 7} {
        \draw[thick, blue] (\i,0) circle (3pt);
    }
    \node at (7,0) [below=3pt] {$x_N$};

    % Labels for h(x)
    \node at (2,0.8) {$h(x) = -1$};
    \node at (6,0.8) {$h(x) = +1$};
    \draw[thick,blue,->] (4.5,0.4) -- (7,0.4);
```

::::
:::

* Growth Function: Examples
::: columns
:::: {.column width=50%}

- **Positive intervals** on $\bbR$ $x \in [a, b]$
  - $m_{\calH}(N) = {N + 1 \choose 2} + 1 \sim \N^2$
  - Pick 2 distinct intervals out of $N + 1$, and there is a dichotomy with 2
    points in the same interval

::::
:::: {.column width=45%}

```tikz
% Draw axis
\draw[very thick] (-0.5,0) -- (9,0);

% Draw negative samples (crosses)
\foreach \i/\name in {0/x_1, 1/x_2, 2/x_3} {
    \draw[thick, red] (\i,0) node[below=3pt] {$\mathit{\name}$} node {\textsf{x}};
}
\node at (3, -0.3) {$\cdots$};

% Draw positive samples (circles)
\foreach \i in {4, 5, 6} {
    \draw[thick, blue] (\i,0) circle (3pt);
}

% Draw final negative example
\draw[thick, red] (7,0) node {\textsf{x}};
\draw[red] node at (7, -0.3) {$x_N$};

% Draw brackets indicating h(x)=+1 region
\draw[very thick,blue,<->] (3.6,0.5) -- (6.4,0.5);
\draw[very thick,blue,rounded corners] (3.6,0.4) -- (3.6,0.6);
\draw[very thick,blue,rounded corners] (6.4,0.4) -- (6.4,0.6);

% Labels for h(x)
\node at (1.5, 0.9) {\color{red}$h(x) = -1$};
\node at (5, 0.9) {\color{blue}$h(x) = +1$};
\node at (8.3, 0.9) {\color{red}$h(x) = -1$};
```

::::
:::

\vspace{1cm}

::: columns
:::: {.column width=50%}

- **Convex sets on a plane**
  - $m_{\calH}(N) = 2^N$
  - Place points in a circle and can classify $N$ points in any way

::::
:::: {.column width=45%}

```tikz[width=50%]
% Circle radius
\def\r{3}

% Draw the outer circle
\draw[thick] (0,0) circle (\r);

% Draw the shaded polygonal region inside
\fill[gray!20,opacity=0.8]
    ({\r*cos(250)},{\r*sin(250)}) --
    ({\r*cos(290)},{\r*sin(290)}) --
    ({\r*cos(30)},{\r*sin(30)}) --
    ({\r*cos(80)},{\r*sin(80)}) -- cycle;

% Label inside region
\node at (0.7,0) {$h(x) = +1$};

% Draw the points (alternating red circles and blue crosses)
\foreach \i in {0,...,11} {
    \pgfmathsetmacro{\angle}{\i * 30}
    \pgfmathsetmacro{\x}{\r*cos(\angle)}
    \pgfmathsetmacro{\y}{\r*sin(\angle)}
    \ifodd\i
        \node[text=blue] at (\x,\y) {\textsf{x}};
    \else
        \draw[thick, red] (\x,\y) circle (3pt);
    \fi
}
```

::::
:::

* Break Point of an Hypothesis Set
- Given an hypothesis set $\calH$

- A hypothesis set $\calH$ **shatters $N$ points** $\iff$ $m_{\calH}(N) = 2^N$
  - There is a position of $N$ points and a class assignment that you can
    classify using $h \in \calH$
  - It does not mean all sets of $N$ points can be classified in any way

- $k$ is a **break point** for $\calH$ $\iff m_{\calH}(k) < 2^k$
  - I.e., no data set of size $k$ can be shattered by $\calH$
  - E.g.,
    - For 2D perceptron: a break point is 4
    - For positive rays: a break point is 2
    - For positive intervals: a break point is 3
    - For convex set on a plane: there is no break point

* Break Point for an Hypothesis Set and Learning
- If there is a break point for a hypothesis set $\calH$, it can be shown that:

  - $m_{\calH}(N)$ is polynomial in $N$
  - Instead of Hoeffding's inequality for learning
    $$
    \Pr(|E_{in}(g) - E_{out}(g)| > \varepsilon) \le 2 M e^{-2 \varepsilon^2 N}
    $$
    you can use the Vapnik-Chervonenkis inequality:
    $$
    \Pr(\text{bad generalization}) \le
    4 m_\calH(2N) e^{-\frac{1}{8} \varepsilon^2 N}
    $$
   - Since $m_{\calH}(N)$ is polynomial in $N$, it will be dominated by the
     negative exponential, given enough examples
   - You can have a generalization bound: machine learning works!

- A hypothesis set can be characterized from the learning point of view by the
  **existence and value of a break point**

//* What Can Replace $M$ in Hoeffding Inequality
//- How does $m_\calH(N)$ relate to overlaps?
//  - Given an hypothesis $g$, the "bad event" is a function of which data set $D$
//    is used for training
//  - Consider the space of data sets as an area of the plane: for some data sets
//    $|E_{in} - E_{out}| > \varepsilon$, and we color the area representing the
//    data set as bad
//  - Hoeffding tells us that the area representing the bad event for hypothesis
//    $g$ is small
//  - The union bound tells us that the areas representing the bad events for the
//    various hypothesis are not overlapping (even if small) and thus the space is
//    quickly filled, since there are many hypothesis (often infinity)
//  - The VC bound tells us that there is a lot of overlap between the bad events.
//  - The intuition is that if one point is colored by an hypothesis as bad, we
//    know that many others hypothesis, say 100, will color the same paint as bad
//    events, so that the area is 100 smaller than what would have been without
//    overlap
//  - The growth function is a measure of how many hypothesis correspond to the
//    same dichotomy
//
//- What to do about $E_{out}$?
//  - The problem is that the bad event not only is function of $E_{in}$ (which is
//    function of the data set) but also of $E_{out}$
//  which is function of the entire space
//
//- Consider the bin with $E_{in}$ and
//  $E_{out}$. If there are lots of bins $E_{out}$ is going to deviate from
//  $E_{in}$. Instead of picking one sample we pick 2 samples, $E_{in}$ and
//  $E'_{in}$. They both track $E_{out}$ and thus track each other, although
//  loosely
//  - We can characterize the bad events in terms of $E_{in}$ and $E'_{in}$, but
//    one can use only dichotomies to reason about the bad event
//  - For this reason there is $m_\calH(2N)$ in the VC inequality
//
//// TODO: Improve this

# ##############################################################################
# The VC Dimension
# ##############################################################################

* VC Dimension of an Hypothesis Set
- The **VC dimension of a hypothesis set** $\calH$, denoted as $d_{VC}(\calH)$,
  is defined as the largest value of $N$ for which $m_{\calH}(N) = 2^N$
  - I.e., the VC dimension is the most points $\calH$ can shatter

- **Properties** of the VC dimension: if $d_{VC}(\calH) = N$ then

  - Exists a constellation of $N$ points that can be shattered by $\calH$
    - Not all sets of $N$ points can be shattered
    - If $N$ points were placed randomly, they could not be necessarily shattered

  - $\calH$ can _shatter_ $N$ points for any $N \le d_{VC}(\calH)$

  - The _smallest break point_ is $d_{VC} - 1$

  - The _growth function_ in terms of the VC dimension is
    $m_{\calH} \le \sum_{i=0}^{d_{VC}} {N \choose i}$

  - The VC dimension is the _order of the polynomial bounding_ $m_{\calH}$

* VC Dimension: Interpretation
- The VC dimension **measures the complexity** of a hypothesis set in terms of
  **effective parameters**

- E.g.,
  - A perceptron in a $d$-dimensional space has $d_{VC} = d + 1$
  - In fact $d_{VC}$ is the number of perceptron parameters!
  - E.g., for a 2D perceptron ($d = 2$), the break point is 2, so $d_{VC} = 3$

- The VC dimension considers the model as a black box in order to estimate
  effective parameters
  - How many points $N$ a model can shatter, not the number of parameters

- Not all parameters contribute to degrees of freedom
  - E.g., combining $N$ 1D perceptrons gives $2N$ parameters, but the effective
    degrees of freedom remain 2

- A complex hypothesis $\calH$:
  - Has more parameters (higher VC dimension $d_{VC}$)
  - Requires more examples for training

* VC Generalization Bounds
- How many data points are needed to obtain
  $\Pr(|E_{in} - E_{out}| > \varepsilon) \le \delta$?

- The VC inequality states
  $$
  \Pr(\text{bad generalization}) \le
    4 m_\calH(2N) e^{-\frac{1}{8} \varepsilon^2 N}
  $$

::: columns
:::: {.column width=55%}

- $N^d e^{-N}$ abstracts the upper bound term
  - Plot $N^d e^{-N}$ vs. $N$: Power dominates for small $N$, exponential for
    large $N$ and brings it to 0
  - Vary $d$ (VC dimension) function peaks for larger $N$, then approaches
    the region of interest $< 1$

::::
:::: {.column width=35%}

![](msml610/lectures_source/figures/Lesson05_VC_Generalization_Bounds.png)

::::
:::

::: columns
:::: {.column width=55%}

- Plot intersection of $N^d e^{-N}$ with a probability as a function of $d$
  - Examples $N$ needed are proportional to $d$
  - Rule of thumb: $N \ge 10 d_{VC}$ for generalization

::::
:::: {.column width=35%}

![](msml610/lectures_source/figures/Lesson05_VC_Generalization_Bounds2.png)

::::
:::

* VC Generalization Bounds
- The VC inequality 
  $$
  \Pr(|E_{in} - E_{out}| > \varepsilon) \le 
    4 m_\calH(2N) e^{-\frac{1}{8} \varepsilon^2 N}
  $$
  can be used in several ways to relate $\varepsilon$, $\delta$, and $N$, e.g.,

- Examples
  - "Given $\varepsilon$ = 1% error, how many examples $N$ are needed to get
    $\delta = 0.05$?"
  - "Given $N$ examples, what's the probability of an error larger than
    $\varepsilon$?"

- You can equate $\delta$ to $4 m_{\calH}(2N) e^{\frac{1}{8}\varepsilon^2 N}$ and
  solve for $\varepsilon$, getting
  $$
  \Omega(N, \calH, \delta)
  = \sqrt{\frac{8}{N} \ln \frac{4 m_{\calH}(2N)}{\delta}}
  $$
- Then you can say $|E_{out} - E_{in}| \le \Omega(N, \calH, \delta)$ with
  probability $\ge 1 - \delta$
  - The generalization bounds are then:
    $\Pr(E_{out} \le E_{in} + \Omega) \ge 1 - \delta$

* How to Void the VC Analysis Guarantee
::: columns
:::: {.column width=60%}
- Consider the case where data is genuinely non-linear
  - E.g., "o" points in the center and "x" in the corners

- Transform to high-dimensional $\calZ$ with:
  $$
  \Phi: \vx = (x_0, ... , x_d) \rightarrow \vz = (z_0, ... , z_{\tilde{d}})
  $$

- $d_{VC} \le \tilde{d} + 1$; smaller $\tilde{d}$ improves generalization
  - Use $\vz = (1, x_1, x_2, x_1 x_2, x_1^2, x_2^2)$
  - Why not $\vz = (1, x_1^2, x_2^2)$?
  - Why not $\vz = (1, x_1^2 + x_2^2)$?
  - Why not $\vz = (x_1^2 + x_2^2 - 0.6)$?

::::
:::: {.column width=35%}

![](msml610/lectures_source/figures/Lesson05_Void_VC_guarantee.png)
::::
:::

- Some model coefficients were zero and discarded, leaving machine learning the
  rest
  - VC analysis is a warranty, forfeited if data is examined before model
    selection (data snooping)
  - From VC analysis, complexity is that of the initial hypothesis set

# ##############################################################################
# Overfitting
# ##############################################################################

// Overfitting

* Overfitting: Definition

::: columns
:::: {.column width=50%}

- **Overfitting** occurs when the model fits the data more than what is warranted

- Surpass point where $E_{out}$ is minimal (optimal fit)
  - Model complexity too high for data/noise
  - Noise in training set mistaken for signal

- **Fitting noise instead of signal** is not useless but harmful
  - Model infers in-sample pattern that, when extrapolated out-of-sample,
    deviates from target function $\implies$ poor generalization

::::
:::: {.column width=45%}

```tikz
% Axis
\draw[->] (0,0) -- (7,0) node[right] {$\text{VC dimension, } d_{\text{vc}}$};
\draw[->] (0,0) -- (0,5) node[above] {Error};

% Dashed line for optimal VC dimension
\draw[dashed, thick] (2.5,0) -- (2.5,4.5);
\node at (2.5,-0.3) {$d_{\text{vc}}^*$};

% In-sample error curve
\draw[thick, blue] plot[smooth, domain=0.6:6] (\x, {2.0/(0.5*\x^2.0+0.3)});

% Model complexity (square root curve)
\draw[thick, violet] plot[smooth, domain=0.6:6] (\x, {1.0*(\x/1.5)^0.6});

% Out-of-sample error curve (in-sample + model complexity)
\draw[thick, red] plot[smooth, domain=0.6:6] (\x, {2.0/(0.5*\x^2.0+0.3) + 1.0*(\x/1.5)^0.6});

% Labels
\node[blue] at (5.0,0.7) {In-sample Error};
\node[violet] at (5.5,1.5) {Model Complexity};
\node[red] at (6.0,2.8) {Out-of-sample Error};
```

::::
:::

* Optimal Fit

::: columns
:::: {.column width=55%}

- The opposite of overfitting is **optimal fit**
  - Train a model with proper complexity for the data

- The optimal fit:
  - Implies $E_{out}$ is minimal
  - Does not imply generalization error $E_{out} - E_{in}$ is minimal
    - E.g., no training implies generalization error equal to 0

- The **generalization error** is the additional error $E_{out} - E_{in}$ when
  going from in-sample to out-of-sample

::::
:::: {.column width=40%}

![](msml610/lectures_source/figures/Lesson05_Optimal_fit.png)

::::
:::

* Overfitting: Diamond Price Example
- Predict diamond price as a function of carat size (regression problem)

- **True relationship**
  $$
  \text{price} \sim (\text{carat size})^2 + \varepsilon
  $$
  where:
  - Square function: price increases more with rarity
  - Noise $\varepsilon$: e.g., market noise, missing features

::: columns
:::: {.column width=50%}

- **Fit with:**
  - _Line_
    - Underfit
    - High bias (large error)
    - Low variance (stable model)
  - _Polynomial of degree 2_
    - right fit
  - _Polynomial of degree 10_
    - Overfit (wiggly curve)
    - Low bias
    - High variance (many degrees of freedom)
::::
:::: {.column width=45%}

![](msml610/lectures_source/figures/Lesson05_Diamond_price_example.png)

::::
:::

* Overfitting: Classification Example
- Assume:
  - You want to separate 2 classes using 2 features $x_1, x_2$
  - The true class boundary has a parabola shape

- You can use logistic regression and a decision boundary equal to:
  - A line $\text{logit}(w_0 + w_1 x + w_2 y)$
    - Underfit
    - High bias, low variance
  - A parabola $\text{logit}(w_0 + w_1 x + w_2 x^2 + w_3 x y + w_4 y^2)$
    - Right fit
  - A wiggly decision boundary
    $\text{logit}(w_0 + \text{high powers of } x_1, x_2)$
    - Overfit
    - Low bias, high variance

![](msml610/lectures_source/figures/Lesson05_Optimal_fit2.png){ width=80% }

# ##############################################################################
# Bias Variance Analysis
# ##############################################################################

* VC Analysis vs Bias-Variance Analysis
- Both VC analysis and bias-variance analysis are concerned with the hypothesis
  set $\calH$
  - **VC analysis**:
    $$
    E_{out} \le E_{in} + \Omega(\calH)
    $$
  - **Bias-variance analysis**
    $$
    E_{out} = \text{bias + variance}
    $$

// TODO(gp): Redraw this

![](msml610/lectures_source/figures/Lesson05_VC_analysis_vs_Bias_Variance_analysis.png){ width=80% }

* Hypothesis Set and Bias-Variance Analysis
- **Learning** consists in finding $g \in \calH$ such that $g \approx f$ where
  $f$ is an unknown function

- The **tradeoff in learning** is between:
  - Bias vs variance
  - Overfitting vs underfitting
  - More complex vs less complex $\calH$ / $h$
  - Approximation (in-sample) vs generalization (out-of-sample)

* Decomposing Error in Bias-Variance (1/4)
- **Problem**
  - Regression set-up: target is a real-valued function
  - Hypothesis set $\calH = \{ h_1(\vx), h_2(\vx), ... h_n(\vx) \}$
  - Training data $\calD$ with $N$ examples
  - Squared error $E_out = \EE[(g(\vx) - f(\vx))^2]$
  - Choose the best function $g \in \calH$ that approximates unknown $f$

- **Question**
  - What is the out-of-sample error $E_{out}(g)$ as function of $\calH$ for a
    training set of $N$ examples?

* Decomposing Error in Bias-Variance (2/4)
- The final hypothesis $g$ depends on training set $D$, so make the dependency
  explicit $g^{(D)}$:
  $$
  E_{out}(g^{(D)})
  \defeq \EE_{\vx}[ ( g^{(D)}(\vx) - f(\vx)) ^ 2 ]
  $$

- Interested in:
  - Hypothesis set $\calH$ rather than specific $h$
  - Training set $D$ of $N$ examples, rather than a specific $D$

- Remove dependency from $D$ by averaging over all possible training sets $D$
  with $N$ examples:
  $$
  E_{out}(\calH)
  \defeq \EE_{D}[ E_{out}(g^{(D)}) ]
  = \EE_{D}[\EE_{\vx}[ ( g^{(D)}(\vx) - f(\vx)) ^ 2 ]]
  $$

* Decomposing Error in Bias-Variance (3/4)
- Switch the order of the expectations since the quantity is non-negative:
  $$
  E_{out}(\calH)
  = \EE_{\vx}[ \EE_{D}[ ( g^{(D)}(\vx) - f(\vx)) ^ 2 ]
  $$

- Focus on $\EE_D [( g^{(D)}(\vx) - f(\vx) ) ^ 2 ]$ which is a function of $\vx$

- Define the \textit{average hypothesis} over all training sets as:
  $$
  \overline{g}(\vx) \defeq \EE_D [ g^{(D)} (\vx) ]
  $$

- Add and subtract it inside the $\EE_D$ expression:
  $$
  \begingroup \small
  \begin{aligned}
  E_{out}(\calH)
  = & \EE_{\vx} \left[ \EE_D \left[ \left(
  g^{(D)}(\vx) - f(\vx)
  \right) ^ 2 \right] \right]\\
  %
  = & \EE_{\vx} \EE_D [ (
  g^{(D)} - \overline{g} +
  \overline{g} - f
  ) ^ 2 ]\\
  = & \EE_{\vx} \EE_D [
  (g^{(D)} - \overline{g}) ^ 2 +
  (\overline{g} - f ) ^ 2 +
  2 (g^{(D)} - \overline{g}) (\overline{g} - f)
  ]\\
  %
  & (\EE_D \text{ is linear and } (\overline{g} - f )
  \text{ doesn't depend on } D) \\
  %
  = & \EE_{\vx} \left[
  \EE_D[(g^{(D)} - \overline{g}) ^ 2] +
  (\overline{g} - f ) ^ 2 +
  2 \EE_D[(g^{(D)} - \overline{g})] (\overline{g} - f)
  \right]\\
  \end{aligned}
  \endgroup
  $$

* Decomposing Error in Bias-Variance (4/4)
- The cross term:
  $$
  \EE_D[(g^{(D)} - \overline{g})] (\overline{g} - f)
  $$
  disappears since applying the expectation on $D$, it is equal to:
  $$
  (g^{(D)} - \EE_D[\overline{g}]) (\overline{g} - f)
  = 0 \cdot (\overline{g} - f)
  = 0 \cdot \text{constant}
  $$

- Finally:
  \begin{alignat*}{3}
  E_{out}(\calH)
  & = \EE_{\vx} [ \EE_D [ ( g^{(D)} - \overline{g} ) ^ 2 ] +
  ( \overline{g}(\vx) - f(\vx) ) ^ 2 ]
  &
  \\
  & = \EE_{\vx} [ \EE_D [ ( g^{(D)} - \overline{g} ) ^ 2 ] ] +
  \EE_{\vx}[( \overline{g} - f) ^ 2] 
  & (\EE_{\vx} \text{ is linear}) \\
  & = \EE_{\vx} [ \text{var}(\vx) ] + \EE_{\vx} [ \text{bias}(\vx)^2 ]
  &
  \\
  & = \text{variance} + \text{bias}
  &
  \\
  \end{alignat*}

// TODO: Add a numerical example

* Interpretation of Average Hypothesis
- The **average hypothesis** over all training sets
  $$
  \overline{g}(\vx) \defeq \EE_D [ g^{(D)} (\vx) ]
  $$
  can be interpreted as the "best" hypothesis from $\calH$ training on $N$
  samples
  - Note: $\overline{g}$ is not necessarily $\in \calH$

- In fact it's like **ensemble learning**:
  - Consider all the possible data sets $D$ with $N$ samples
  - Learn $g$ from each $D$
  - Average all the hypotheses

* Interpretation of Variance and Bias Terms
- The out-of-sample error can be decomposed as:
  $$
  E_{out}(\calH) = \text{bias}^2 + \text{variance}
  $$

::: columns
:::: {.column width=60%}

- **Bias term**
  $$
  \text{bias}^2 = \EE_{\vx} [ ( \overline{g}(\vx) - f(\vx) ) ^ 2 ]
  $$
  - Does not depend on learning as it is not a function of the data set $D$
  - Measures how limited $\calH$ is
    - I.e., the ability of $\calH$ to approximate the target with infinite
      training sets

- **Variance term**
  $$
  \text{variance} = \EE_{\vx} \EE_D [ ( g^{(D)}(\vx) - \overline{g}(\vx) ) ^ 2]
  $$
  - Measures variability of the learned hypothesis from $D$ for any $\vx$
    - With infinite training sets, we could focus on the "best" $g$, which is
      $\overline{g}$
    - But we have only one data set $D$ at a time, incurring a cost
::::
:::: {.column width=35%}

![](msml610/lectures_source/figures/Lesson05_bias_variance.png)

![](msml610/lectures_source/figures/Lesson05_bias_variance_decomposition.png)

::::
:::

* Variance and Bias Term Varying Cardinality of $\calH$

::: columns
:::: {.column width=60%}

- If hypothesis set **has a single function**: $\calH = \{ h \ne f \}$
  - Large bias
    - $h$ might be far from $f$
  - Variance = 0
    - No cost in choosing hypothesis
::::
:::: {.column width=35%}

![](msml610/lectures_source/figures/Lesson05_Bias_Variance_tradeoff_example1.png)

::::
:::

::: columns
:::: {.column width=60%}

\vspace{1cm}

- If hypothesis set **has many functions**:
  $\calH = \{ \text{many hypotheses } h \}$
  - Bias can be 0
    - E.g., if $f \in \calH$
  - Large variance
    - Depending on data set $D$, end up far from $f$
    - Larger $\calH$, farther $g$ from $f$
::::
:::: {.column width=35%}

![](msml610/lectures_source/figures/Lesson05_Bias_Variance_tradeoff_example2.png)

::::
:::

// TODO: Add pic (see book)

* Bias-Variance Trade-Off: Numerical Example
- **Machine learning problem**:
  - Target function $f(x) = \sin(\pi x), x \in [-1, 1]$
  - Noiseless target
  - You have $f(\vx)$ for $N = 2$ points

- **Two hypotheses sets** $\calH$:
  - Constant model: $\calH_0: h(x) = b$
  - Linear model: $\calH_1: h(x) = ax + b$

- **Which model is best**?
  - Depends on the perspective!
  - Best for _approximation_: minimal error approximating the sinusoid
  - Best for _learning_: learn the unknown function with minimal error from 2
    points

* Bias-Variance Trade-Off: Numerical Example

- **Approximation**
  - $E_{out}(g_0) = 0.5$
    - $g_0$ is a constant and approximates the sinusoid poorly (higher bias)
  - $E_{out}(g_1) = 0.2$
    - $g_1$ is a line and has more degrees of freedom (lower bias)
  - The line model _approximates better_ than the constant model

::: columns
:::: {.column width=60%}
- **Learning**
  - Algorithm:
    - Pick 2 points as training set $D$
    - Learn $g$ from $D$
    - Compute $\EE_D [ E_{out}(g) ]$
  - Average over all data sets $D$:
    $$
    E_{out} = \text{bias}^2 + \text{variance}
    $$
  - $E_{out}(g_0) = 0.5 + 0.25 = 0.75$
    - $g_0$ is more stable given the data set (lower variance)
  - $E_{out}(g_1) = 0.2 + 1.69 = 1.9$
    - $g_1$ heavily depends on the training set (higher variance)
  - The constant model _learns better_ than the line model

::::
:::: {.column width=35%}

![](msml610/lectures_source/figures/Lesson05_Bias_Variance_Numerical_Example1.png)
![](msml610/lectures_source/figures/Lesson05_Bias_Variance_Numerical_Example2.png)

::::
:::

* Bias-Variance Curves
- **Bias-variance curves** are plots of $E_{out}$ increasing the complexity of
  the model

::: columns
:::: {.column width=50%}
- Typical **shape** of bias-variance curves
  - $E_{in}$ and $E_{out}$ start from the same point
  - $E_{in}$
    - Is decreasing with increasing model complexity
    - Can even go to 0
  - $E_{out}$
    - Is always larger than $E_{in}$
    - Is the sum of bias and variance
    - Has a bowl shape
    - Reaches a minimum for optimal fit
    - Before the minimum there is a "high bias / underfitting" regime
    - After the minimum there is a "high variance / overfitting" regime
::::
:::: {.column width=45%}

![](msml610/lectures_source/figures/Lesson05_bias_variance_curve.png)

![](msml610/lectures_source/figures/Lesson05_bias_variance_decomposition.png)

::::
:::

* Bias-Variance Curves and Regularization

- Use **model with regularization** to learn at the same time:
  - Model coefficients $\vw$
  - Model "complexity" (e.g., VC dimension)

- Learn the optimal model $\vw(\lambda)$ as a function of
  $\lambda = \{..., 10^{-1}, 1.0, 10, ...\}$ by optimizing:
  $$
  \vw(\lambda) = \argmin_{\vw} E_{aug}(\vw) = E_{in}(\vw) + \Omega(\lambda)
  $$

::: columns
:::: {.column width=45%}

- Interpret regularization parameter:
  - Small $\lambda$:
    - Complex model
    - Low bias
    - High variance
  - Large $\lambda$:
    - Simple model
    - High bias
    - Low variance
  - An intermediate $\lambda$ is optimal

::::
:::: {.column width=50%}

![](msml610/lectures_source/figures/Lesson05_regularization.png)

::::
:::

* How to Measure the Model Complexity
- Number of features
- Parameters for model form / degrees of freedom, e.g.,
  - VC dimension $d_{VC}$
  - Degree of polynomials
  - $k$ in KNN
  - $\nu$ in NuSVM
- Regularization param $\lambda$
- Training epochs for neural network

* Bias-Variance Decomposition with a Noisy Target
- Extend bias-variance decomposition to **a noisy target**
  $$
  y = f(\vx; \vw) + \varepsilon = \vw^T \vx + \varepsilon
  $$

- With similar hypothesis and analysis conclude that:
  $$
  \begin{aligned}
  E_{out}(\calH)
  &= \EE_{D, \vx} \left[
  ( g^{(D)} - \overline{g} )^2
  \right]
  + \EE_{\vx} \left[
  ( \overline{g} - f) ^ 2
  \right]
  + \EE_{\varepsilon, \vx} \left[
  ( f - y) ^ 2
  \right]\\
  &= \text{variance + bias + stochastic noise}\\
  \end{aligned}
  $$

- The out-of-sample error is the sum of 3 contributions
  1. **Variance**: from the set of hypotheses to the centroid of the hypothesis
     set
  2. **Bias**: from the centroid of the hypothesis set to the noiseless function
  3. **Noise**: from the noiseless function to the real function

* Bias as Deterministic Noise
::: columns
:::: {.column width=65%}
- The bias term can be interpreted as **deterministic noise**

- **Bias** is the part of the target function that hypothesis set cannot capture:
  $$
  h^*(\vx) - f(\vx)
  $$
  where:
  - $h^*()$ is the best approximation of $f(\vx)$ in the hypothesis set
    $\calH$ (e.g., $\overline{g}(x)$)

- The hypothesis set $\calH$ cannot learn the deterministic noise since it is
  outside of its ability, and thus it behaves like "noise"

::::
:::: {.column width=30%}

![](msml610/lectures_source/figures/Lesson05_Deterministic_Noise.png)

::::
:::

* Deterministic vs Stochastic Noise in Practice
- **Deterministic noise:**
  - Fixed for a particular $\vx$
  - Depends on $\calH$
  - Independent of $\varepsilon$ or $D$

- **Stochastic noise:**
  - Not fixed for $\vx$
  - Independent of $D$ or $\calH$

- In a machine learning problem, no difference exists between stochastic and
  deterministic noise, as $\calH$ and $D$ are fixed
  - From the training set alone, you cannot determine if data is from a
    _noiseless complex_ target or a _noisy simple_ target

* Deterministic vs Stochastic Noise Example
::: columns
:::: {.column width=70%}
- **Two targets**:
  - Noisy low-order target (10th order polynomial)
  - Noiseless high-order target (50th order polynomial)

- **Training set**
  - Generate $N=15$ data points

- **Two models**
  - ${\calH}_{2}$ low-order hypothesis (2nd order polynomial)
  - ${\calH}_{10}$ high-order hypothesis (10th order polynomial)

- Learning model:
  - Sees only training samples
  - Can't distinguish noise sources

::::
:::: {.column width=25%}

![](msml610/lectures_source/figures/Lesson05_deterministic_noise_example1.png)

![](msml610/lectures_source/figures/Lesson05_deterministic_noise_example2.png)

::::
:::

* Deterministic vs Stochastic Noise Example
::: columns
:::: {.column width=70%}

- Noisy low-order target:
  - Fit 2nd to 10th order polynomial
  - $\downarrow E_{in}$ (more degrees of freedom), $\uparrow\uparrow E_{out}$
    (fits noise)

- Noiseless high-order target
  - Fit 2nd to 10th order polynomial
  - Same phenomenon
  - $\downarrow E_{in}$ (more degrees of freedom), $\uparrow\uparrow E_{out}$
    (fits noise)

- **Wrong approach**
  - Target is 10th order polynomial
  - Use 10-order hypothesis to fit target perfectly

- **Right approach**
  - Consider number of data points, i.e., 15 points
  - Rule of thumb from VC analysis
    $$
    \text{degrees of freedom of model = number of data points / 10}
    $$
  - Use 1 or 2 degrees of freedom

::::
:::: {.column width=25%}

![](msml610/lectures_source/figures/Lesson05_deterministic_noise_example3.png)

![](msml610/lectures_source/figures/Lesson05_deterministic_noise_example5.png)

![](msml610/lectures_source/figures/Lesson05_deterministic_noise_example4.png)

![](msml610/lectures_source/figures/Lesson05_deterministic_noise_example6.png)

::::
:::

* Amount of Data and Model Complexity
- One must match the _model complexity_ to
  - **Data resources**
  - **Signal to noise ratio**
  - **Not target complexity**

- The rule of thumb is:
  $$
  d_{VC} (\text{degrees of freedom of the model})
  = N (\text{number of data points}) / 10
  $$
  - In other words, 10 data points needed to fit a degree of freedom
  - If the data is noisy, you need even more data

* Overfitting as a Function of Data Resources, Model Complexity, Noise

- You can measure overfitting in terms of **generalization error**
  $\frac{E_{out} - E_{in}}{E_{out}}$

  - $\uparrow$ data resources $N \implies \downarrow$ overfitting
  - $\uparrow$ model complexity $d_{VC} \implies \uparrow$ overfitting
  - $\uparrow$ deterministic noise (target complexity) $\implies \uparrow$ overfitting
  - $\uparrow$ stochastic noise $\sigma^2 \implies \uparrow$ overfitting

# ##############################################################################
# Learning Curves
# ##############################################################################

* Learning Curves vs Bias-Variance Curves
- **Learning curves** are the dual of the bias-variance curves

- For **bias-variance curves**
  $$
  E_{in}, E_{out} = f(d_{VC} \vert N)
  $$
  - Keep the complexity of training set fixed (number of examples $N$)
  - Vary the model in terms of:
    - Model complexity $d$
    - Number of features $p$
    - Regularization amount $\lambda$

::: columns
:::: {.column width=55%}
- For **learning curves**
  $$
  E_{in}, E_{out} = f(N \vert d_{VC})
  $$
  - Fix the model
  - Vary the size $N$ of training set

::::
:::: {.column width=40%}

```tikz
% Axes
\draw[->] (0,0) -- (7,0) node[below] {Number of Data Points, $N$};
\draw[->] (0,0) -- (0,4) node[above] {Expected Error};

% Dotted convergence line
\draw[dotted, thick] (0,1.5) -- (6.8,1.5);

% E_in curve
\draw[thick, blue] plot[smooth, domain=0.4:6.5] (\x, {1.5 + 1.2/(0.6*\x + 0.4)});

% E_out curve
\draw[thick, red] plot[smooth, domain=0.5:6.5] (\x, {1.5 - 1.0/(0.6*\x + 0.4)});

% Labels
\node[red] at (5.8,0.95) {$E_{\text{in}}$};
\node[blue] at (5.8,2.15) {$E_{\text{out}}$};
```
::::
:::

* Typical Form of Learning Curves
- Learning curves plot $E_{in}$ and $E_{out}$ as a function of data amount $N$,
  given the model $h()$
  - $E_{out} \ge E_{in}$ for any $N$

::: columns
:::: {.column width=60%}

- **Small $N$**
  - $E_{in}$ is small (even 0)
  - The model is likely overfitted, memorizing and generalizing poorly
  - $E_{out}$ is large

- **Increasing $N$**
  - $E_{in} \uparrow$ as the model cannot fit all data
  - $E_{out} \downarrow$ as the model fits better and generalizes better
  - Generalization error $E_{out} - E_{in} \downarrow$

- **$N \to \infty$**
  - $E_{in}$ reaches a maximum (irreducible error)
  - $E_{out}$ reaches a minimum
  - $E_{out} - E_{in}$ depends on the complexity of the model
::::
:::: {.column width=35%}

![](msml610/lectures_source/figures/Lesson05_learning_curve.png)

![](msml610/lectures_source/figures/Lesson05_learning_curve2.png)
::::
:::

* High-Bias vs High-Variance Regime
::: columns
:::: {.column width=60%}
- From the learning curve you can see the two regimes:

  - **High-variance regime** for small $N$
    - $E_{in}$ is small
    - Small data set $D$ $\implies$ high dependency on $D$

  - More data helps reduce gap between $E_{in}$ and $E_{out}$

  - **High-bias regime** for large $N$
    - $E_{in}$ is large; flattens for large $N$
    - Best model fitted; more data won't help
    - $E_{out}$ can be close to $E_{in}$ (good generalization) or not
::::
:::: {.column width=35%}

![](msml610/lectures_source/figures/Lesson05_Irreducible_Error1.png)

![](msml610/lectures_source/figures/Lesson05_Irreducible_Error2.png)
::::
:::

// TODO: Add pic

//* Example of learning curves for linear regression
//- Consider a noisy target
//
//  $$
//  y = \vw^T \vx + \varepsilon
//  $$
//
//- The optimal estimate of $\vw$ is given by:
//
//  $$
//  \hat{\vw} = (\mX^T \mX)^{-1} \mX^T \vy
//  $$
//
//  where the measured $\vy = \mX \vw + \vvarepsilon$, so
//
//  $$
//  \begin{aligned}
//  \hat{\vw}
//  &= (\mX^T \mX)^{-1} \mX^T (\mX \vw + \vvarepsilon) \\
//  &= (\mX^T \mX)^{-1} \mX^T \mX \vw +
//  \mX^T \mX)^{-1} \mX^T \vvarepsilon \\
//  &= \vw + \mX^T \mX)^{-1} \mX^T \vvarepsilon \\
//  \end{aligned}
//  $$
//
//- We can compute in-sample error by replacing the measured $\vy$ and the
//  estimated $\vw$:
//
//  $$
//  \begin{aligned}
//  E_{in}
//  &= \|\vy - \mX \hat{\vw} \|^2 \\
//  &= \| (\mX \vw + \vvarepsilon) -
//  (\mX \vw +
//  \mX (\mX^T \mX)^{-1} \mX^T \vvarepsilon) \\
//  &= \| \vvarepsilon -
//  \mX (\mX^T \mX)^{-1} \mX^T \vvarepsilon) \\
//  &= \| (\mI - \mX (\mX^T \mX)^{-1} \mX^T) \vvarepsilon) \\
//  \end{aligned}
//  $$
//
//- Out-sample error: $E_{out} = \|\vy' - \mX \hat{\vw}\|^2$ using a different
//  realization of the noise
//
//- For $N \le d + 1$, $E_{in} = 0$ since we have $d + 1$ degrees of freedom and
//  we can capture signal and noise perfectly
//- Then $E_{in}$ increases and goes towards $\sigma ^ 2$ which is related to the
//  noise since we cannot capture it with out hypothesis set
//
//- For linear regression we can compute analytically the solution
//  $$
//  \begin{aligned}
//  & E_{in} = \sigma^2 (1 - \frac{d + 1}{N}) \\
//  & E_{out} = \sigma^2 (1 + \frac{d + 1}{N}) \\
//  \end{aligned}
//  $$
//- TODO: Why these relationships?
//- The generalization error is: $E_{out} - E_{in} = 2 \sigma^2 (\frac{d + 1}{N})$
//  which shows that what matters is $\frac{d + 1}{N}$ (our rule of thumb was
//  $N > 10 d_{VC}$)

# ##############################################################################
# Learn-Validation Approach
# ##############################################################################

## #############################################################################
## Train / Test
## #############################################################################

* Estimating Out-Of-Sample Error with One Point
- Pick a **single out-of-sample point** $(\vx', y)$

- The error of the model $h$ is:
  $$
  E_{val}(h) = e(h(\vx'), y)
  $$
  where the error can be:
  - Squared error $(h(\vx) - y)^2$
  - Binary error $I[h(\vx) - y]$
  - ...

- The **error on an out-of-sample point** is an **unbiased estimate** of
  $E_{out}$, since:
  $$
  \EE[E_{val}(h)] = \EE[e(h(\vx), y)] = E_{out}
  $$
- The quality of the estimate depends on the standard error $\VV[e(h(\vx), y)]$,
  which in an unknown value

* Estimating Out-Of-Sample Error with $K$ Points
- To improve the estimate, use a validation set with $K$ points 
  $(\vx_1, y_1), ... , (\vx_K, y_K)$ drawn IID

- Compute the **error on the validation set** as:
  $$
  E_{val}(h) = \frac{1}{K} \sum_{i=1}^K e(h(\vx_i), y_i)
  $$

- The validation error is an **unbiased estimate** of out-of-sample error since:
  $$
  \EE[E_{val}(h)] = E_{out}(h)
  $$
- The standard error is $\sqrt{K}$ smaller:
  \begingroup \small
  \begin{alignat*}{2}
  \VV[E_{val}(h)]
  & = \frac{1}{K^2} \sum_i \VV[e(h(\vx_i), y_i)] + \text{covariances}
  \\
  &\text{(covariances are 0 because $\vx_i$ independent)}
  \\
  &= \frac{1}{K^2} \sum_i \VV[e(h(\vx_i), y_i)]
  \\
  &= \frac{K \sigma^2}{K^2} = \frac{\sigma^2}{K}
  \\
  \end{alignat*}
  \endgroup

* Trade-Off Between Training and Validation Set
- **Problem**: you have finite amount of data points $N$

::: columns
:::: {.column width=60%}
  $$
  \begin{aligned}
  & D_{val} = \{K \text{ points}\} \\
  & D_{train} = \{N - K \text{ points}\} \\
  \end{aligned}
  $$

- You know that:
  $$
  \begin{aligned}
  \uparrow K
  & \implies |E_{val} - E_{out}| \downarrow \text{ (most reliable estimate)}
  \\
  & \implies \downarrow N - K \implies E_{in}, E_{out} \uparrow \text{ (worse model)}
  \\
  \end{aligned}
  $$
  - If $K$ is too big, you get a **reliable estimate of a bad number**!

::::
:::: {.column width=35%}

![](msml610/lectures_source/figures/Lesson05_Train_Validation_Set_Trade_Off.png)

::::
:::

- **Solution**:
  - Rule of thumb: 70-30 or 80-20 split between train and validation

* Error From VC Analysis vs Learn-Validation Approach
- In general:
  $$
  E_{out}(h) = E_{in}(h) + \text{generalization error}
  $$

- **VC analysis**
  - Estimates generalization error as "overfit penalty" in terms of hypothesis
    set complexity

- **Learn-validation**
  - Estimates $E_{out}$ directly by holding out data as validation set:
    $$
    E_{val} \approx E_{out}
    $$
  - Use learn-validation approach at different points of the modeling flow:
    - For selecting hyperparameters (validation set)
    - To estimate final performance (test set)

* Reusing Validation / Test Set for Training
- Any data used for learning (e.g., model training or model selection)
  - Is biased and optimistic
  - Cannot used to assess the learned model

- **Never use** $D_{val}$ / $D_{test}$ for training or $D_{train}$ for evaluation

- After research is done, all available data can be used to learn

::: columns
:::: {.column width=60%}

- **Algorithm**
  1. Train with $N - K$ points to learn $g^-$
  2. Use $K$ points to compute $E_{val}[g^-]$ estimating $E_{out}[g^-]$
  3. Once the model form is finalized, use all $N$ data points (including
     validation, test set) to re-train to get $g$
     - $E_{val}[g] < E_{val}[g^-]$ since $g$ is learned on a larger data set
       than $g^-$ and is thus better
  4. Deliver to customers:
     - Final hypothesis $g$
     - Upper bound of out-of-sample performance $E_{val}[g^-]$

::::
:::: {.column width=35%}

![](msml610/lectures_source/figures/Lesson05_Use_Validation_Set_To_Train.png)

::::
:::

// TODO: Add pic

* Learn-Validation Approach: Pros and Cons
- **Pros**
  - Estimate $E_{out}$ using $E_{val}$
  - Simple to compute, no complexity from VC analysis

- **Cons**
  - Cannot use all data for learning and validation; need a compromise
  - Learned model and $E_{val}$ depend on the split; different splits can give
    different results

## #############################################################################
## Cross-Validation
## #############################################################################

* Cross-validation
::: columns
:::: {.column width=55%}

- **Divide dataset** into $K$ folds, each with $\frac{N}{K}$ samples
  - Each fold should reflect dataset statistics
  - E.g., stratified sampling

- **Do $K$ iterations** $i = 1, ..., K$
- In $i$-th iteration
  - Train on all folds except $i$ using $\frac{K - 1}{K}N$ points and get
    $g^{(-i)}(\vx)$
  - Validate on $i$-th fold using $\frac{N}{K}$ points to compute
    $$
    E_{val}^{(i)} = E_{val}[g^{(-i)}(\vx)]
    $$

- **Average** $K$ error rates and compute bounds:
  $$
  E_{val} = \frac{1}{K} \sum_i E_{val}^{(i)}
  $$

::::
:::: {.column width=40%}

\begingroup \scriptsize
_Train / test validation_
\endgroup
\vspace{0.1cm}

```tikz
    \def\blockwidth{1.2}
    \def\blockheight{0.7}

    % Label
    \node[anchor=east] at (-0.2, -0.5*\blockheight) {\textbf{Train/Test Split}};

    % Create 6 blocks
    \foreach \j in {0,...,5} {
        \pgfmathsetmacro{\x}{\j * \blockwidth}
        \pgfmathsetmacro{\y}{0}

        % Mark first 4 as train, last 2 as test
        \ifnum\j<4
            \fill[blue!70] (\x, \y) rectangle ++(\blockwidth, -\blockheight);
            \node at (\x + 0.5*\blockwidth, \y - 0.5*\blockheight) {\textbf{Train}};
        \else
            \fill[red!80] (\x, \y) rectangle ++(\blockwidth, -\blockheight);
            \node at (\x + 0.5*\blockwidth, \y - 0.5*\blockheight) {\textbf{Test}};
        \fi

        \draw[black] (\x, \y) rectangle ++(\blockwidth, -\blockheight);
    }
```

\vspace{1cm}
\begingroup \scriptsize
_5x cross-validation_
\endgroup
\vspace{0.1cm}

```tikz
\def\blockwidth{1.2}
\def\blockheight{0.7}
\def\vspacing{1.2} % vertical spacing between iterations
\def\nfolds{6}

\foreach \i in {1,...,5} {
    % Compute vertical center for this row
    \pgfmathsetmacro{\ycenter}{-\i * \vspacing - 0.5 * \blockheight}

    % Label each iteration (vertically centered)
    \node[anchor=east] at (-0.2, \ycenter) {\textbf{Iteration \i}};

    \foreach \j in {0,...,4} {
        \pgfmathtruncatemacro{\jj}{\j + 1}

        % Compute block position
        \pgfmathsetmacro{\x}{\j * \blockwidth}
        \pgfmathsetmacro{\y}{-\i * \vspacing}

        % Fill and label blocks
        \ifnum\i=\jj
            \fill[red!80] (\x, \y) rectangle ++(\blockwidth, -\blockheight);
            \node at (\x + 0.5*\blockwidth, \y - 0.5*\blockheight) {\textbf{Test}};
        \else
            \fill[blue!70] (\x, \y) rectangle ++(\blockwidth, -\blockheight);
            \node at (\x + 0.5*\blockwidth, \y - 0.5*\blockheight) {\textbf{Train}};
        \fi

        % Draw borders
        \draw[black] (\x, \y) rectangle ++(\blockwidth, -\blockheight);
    }
}
```

:::: 
:::

* Cross-Validation: Pros and Cons
- **Pros**
  - Efficient data usage
    - All data used for learning and validation
  - Better estimate of $E_{val}$ than separate training/validation sets
  - Folds can be stratified

- **Cons**
  - Computationally intensive: $K$ learning phases
  - Dependency on fold selection
  - Errors $E_{val}$ are not independent
    - Coupling through common training samples
    - Experimentally not completely correlated

* Repeated Cross-Validation
- **Repeated Cross-Validation**
  - Cross-validation results depend on fold selection
  - To remove this dependency, repeat cross-validation multiple times (e.g., 10)
    and average results
  - Note that _"10x 10-fold cross-validation"_ is different than _"1x 100-fold
    cross-validation"_

* Leave-One-Out Cross-Validation
- **Leave-one-out** (LOO) cross-validation
  - There are $N$ training sessions
  - Each session trains on $N - 1$ points and validates on 1 point
  - Equivalent to "$N$-fold cross-validation" where $N$ is the number of examples
    in the dataset

- **Train**
  - For $i$-th fold, $N - 1$ samples for training $\implies g^{(-i)} \approx g$

- **Validate**
  - Estimate the validation error on a single point (bad):
    $$
    E_{val}[g^{(-i)}] = e(g^{(-i)}(\vx_i), y_i) \not\approx E_{out}[g^{(-i)}]
    $$
  - Average $E_{val}$ over the points as estimate of $E_{out}$ (good):
    $$
    E_{val}
    = \frac{1}{N} \sum_{i=1}^K E_{val}[g^{(-i)}]
    $$

* Leave-One-Out Cross-Validation: Pros and Cons
- **Pros**
  - Max data used for training
  - Deterministic procedure
    - No dependency on fold selection

- **Cons**
  - High computational cost (as many learning phases as data points)
  - Cannot be stratified
  - Higher correlation between cross-validation estimates

* Bootstrap
- **Algorithm**
  - Pick $N$ samples with replacement from a data set with $N$ instances to
    build training set
  - Pick the elements never chosen to build the test set ("out-of-bag" samples)
  - Training set contains 63.2\% of all the samples, 36.8\% in the test set

- **Pros**
  - Works for small data sets since it "expands" the data

- **Cons**
  - Not flexible
  - Smaller percentage of instances are used for training set than 10-fold cross
    validation

* Bootstrap: Problem
- **Assumption**
  - Given $X \sim F$
  - Draw $n$ IID samples $X_i$ from $F$

- **Goal**:
  - Consider a sampling statistic $T = g(X_1, ..., X_n)$
    - E.g., mean, median, OLS coefficients, ...
  - You want to:
    - Estimate distribution of $T$, e.g., $F_T(x), f_T(x)$
    - Estimate a statistic of $T$, e.g., mean, std err
    - Construct confidence intervals, e.g., $\mu \pm \epsilon$
    - Calculate standard errors, e.g., $\sigma(\hat{T}_n)$
    - Hypothesis testing

* Bootstrap Procedure: Algorithm
- Use observed data $X_1, ..., X_n$ to construct estimated population
  distribution $\hat{F}_n$

- Repeat $B$ times:
  - Draw $n$ samples with replacement from estimated population distribution
    $\hat{F}_n$
  - Sampling with replacement from $X_i$ equals sampling from $\hat{F}_n$
  - Compute sample statistics from $n$ samples:
    $T^{(i)} = g(X^{(i)}_1, ..., X^{(i)}_n)$

- Use $B$ samples of $T^{(i)}$ to estimate empirical distribution $\hat{T}$

- Compute statistics (e.g., confidence interval, standard error) of $T$ from
  empirical distribution of $\hat{T}$

* Bootstrap: Pros
- **Tremendously useful tool**
  - Fewer assumptions
    - No need for simplifying assumptions for closed formulas
    - Data doesn't need to be Gaussian
  - Generality
    - Applies to any sample statistics, even non-linear ones (e.g., median)

- **Math $\to$ simulation**
  - Bootstrap is a non-parametric method
  - Does not rely on large sample sizes, e.g., Central Limit Theorem (CLT) / Law
    Large Numbers (LLN)
  - Bootstrap frees data scientists from complex math, approximations, and
    asymptotics

* Bootstrap: Example of Die Rolls
- **Problem**
  - Compute distribution of sum of rolling a die 50 times
    $$Y = \sum_{i=1}^{50} X_i = g(X_1, ..., X_{50})$$
  - Sample statistics similar to sample mean, but can be anything

- **Solution**
  1. **By math**
     - If you know PMF of the die, compute distribution using mathematics
     - Theorem of lazy statistician for mean, variance
     - Compute PDF of $Y$ by convolving PDFs
  2. **By sampling** (real or simulated)
     - Roll die 50 times
     - Compute sample statistic
     - Repeat procedure
     - Plot approximate distribution of $Y$
  3. If only 50 samples of die are known $\to$ **bootstrap**

* Bootstrap of the Median: Pseudo-Code

// TODO(gp): Add a way to render it with render_images.py

\begingroup \small
```
def bootstrap_median(x, n_boot):
    # Compute n_boot sample statistics.
    median_boot = [0.0] * n_boot
    for i in range(n_boot):
        # Sample with replacement.
        x_star = sample with replacements from x
        # Compute median for bootstrapped samples.
        median_boot[i] = median(x_star)
    # Compute mean and std err from approximation of sample statistics.
    m_median = numpy.mean(median_boot)
    se_median = numpy.std(median_boot)
    return m_median, se_median
```
\endgroup

* Bootstrap for variance of sample statistics: explanation
- Under the hypotheses of bootstrap:
  - $X \sim F$
  - $n$ samples $X_i$ IID from $F$
  - Statistic $T = g(X_1, ..., X_n)$
  - Compute $\VV_F[T]$ (variance of sample statistics), where $F$ is unknown

- **First approximation**
  - Only samples from $F$ available
  - Approximate $F$ with $\hat{F}$, as empirical CDF $\hat{F}$ converges to $F$
    $$\VV_F[T] \approx \VV_{\hat{F}}[T]$$
  - Approximation
    - Not small
    - Depends on sample size and shape of $F$

- **Second approximation**
  - Exact $\hat{F}$ may lack closed formula for $\VV_{\hat{F}}[T]$
  - Use LLN and simulation to approximate variance:
    $$
    \VV_{\hat{F}}[T]
    \approx v_{boot}
    = \frac{1}{B} \sum_i (T_i - \overline{T}) ^ 2
    $$
  - Approximation size reduced by increasing $B$
  - $v_{boot} \to \VV_{\hat{F}}[T]$ as $B \to \infty$
